\begin{abstract}

 直复营销即一种可以得到客户直接回应的营销模式。作为企业的一项长期性经营活动，直复营销贯穿于企业发展的整个过程，因此，通常将长期收益作为评价营销效果的指标。近年来，随着智能化的快速发展，越来越多的企业希望借助机器学习的力量进行营销决策，但是传统的监督学习和非监督学习方法在处理该问题时只能最大化单个决策的即时收益，而直复营销需要随时间的推移进行连续决策，因而具有很大的局限性。

 强化学习是机器学习的重要组成部分，主要用于解决序贯决策问题。它通过智能体持续地与环境进行交互，并从环境反馈的延迟奖赏中学习状态与行为之间的映射关系，以使得累积奖赏最大化。考虑到直复营销的过程也是一个序贯决策过程，并且其追求的长期收益最大化与强化学习累积奖赏最大化的目标不谋而合，因此，使用强化学习技术解决直复营销决策问题具有天然的优势，这是本文研究的出发点。另外， 为了更好地适应实际需求，本文从基于值函数的强化学习方法着手，针对直复营销场景中营销决策点间的时间间间隔不固定、数据负载大导致学习速度慢以及客户状态的部分可观测等问题，提出相应的改进方法，并使用仿真环境进行评估。具体如下：

 一方面，针对直复营销场景中营销决策点间的时间间隔不固定以及数据规模大导致学习速度慢等问题，本文基于经典的强化学习算法Q-learning进行研究，并提出了改进的Q-learning算法。具体地，使用均值标准化的方法减少因为决策点间时间间隔不固定而给奖赏信号带来的噪声影响，进而又针对Q值函数在迭代过程中因为时间间隔更新不同步而带来的偏差问题，构建一个标准化因子，并仿照值函数的更新方法进行标准化因子的更新，由此提出Interval-Q算法。接着，针对Interval-Q算法在处理大规模数据时，训练速度慢，学习效率不高的问题，本文在Q采样法的基础上，引入时间差分（TD）偏差，提出基于TD偏差的Q采样法。最后，通过仿真实验证明，本文所提的Interval-Q算法在不定期直复营销场景中可以取得更高的收益，另外，基于TD偏差的Q采样法，可以在减少采样数量的同时达到更好的学习效果。 

 另一方面，针对传统强化学习算法无法有效处理直复营销场景中客户状态部分可观测的问题，本文基于深度强化学习DQN模型进行研究，并提出了基于双网络的DQN模型。具体地，首先结合营销场景的时序特点，提出通过使用基于RNN网络的DQN模型（DQN_RNN）以学习隐状态的方式来解决上述问题。然后，指出DQN_RNN模型在网络优化过程中不能很好地同时进行隐状态的学习和值函数的逼近，由此提出基于双网络的DQN模型：通过RNN网络从监督数据中学习隐状态的表示方法，再将RNN网络输出的隐状态信息作为DQN网络的输入状态进行强化学习，通过这种方式可以充分发挥这两个网络各自的优势，在提高值函数逼近效果的同时也能更好地学习隐状态。最后，为了取得更好的策略学习效果，又从网络结构和训练方法两个角度进行分析，提出改进的模型结构：双网络独立训练模型、双网络一步联合训练模型和双网络两步联合训练模型。通过仿真实验证明，本文所提出的基于双网络的DQN模型在定期直复营销场景中可以取得更高的收益。

% ，并且不需要利用大批量数据同样可以产生较好的营销策略。
\keywords{强化学习；值函数；Q-learning算法；深度Q网络；直复营销}
\end{abstract}

\begin{enabstract}

Direct Marketing is a marketing model in which customers can directly respond to companies. As a long-term business activity of companies, Direct Marketing exists in the entire process of companies' development. Therefore, the Long-Term Value (LTV) is usually considered as an indicator to evaluate the marketing effectiveness. In recent years, with the rapid development of intelligence, more and more companies hope to use the technology of Machine Learning (ML) to make marketing decisions. However, the traditional Supervised Learning (SL) and Unsupervised Learning (UL) methods have great limitations when dealing with this problem. The reason behind that is these methods can only maximize the immediate profits of a single decision whereas in Direct Marketing sequences of decisions need to be made over time.

Reinforcement Learning (RL) is an important part of Machine Learning and mainly used to solve sequential decision problems. In the process of RL, the Agent continuously interacts with the environment and learns the mapping relationship between states and actions from delayed rewards of the environment to maximize the cumulative discounted reward. Considering that the process of Direct Marketing is also a sequential decision-making process, and its goal of long-term profit maximization coincides with the goal of maximizing the cumulative discounted reward for Reinforcement learning. Therefore, there is a great advantage to use Reinforcement Learning methods to solve the Direct Marketing decision problems, which is the starting point of this research. In addition, in order to effectively meet the actual requirements, this research starts with Reinforcement Learning methods based on value function. This research aims at three problems, the decision-making time intervals in the Direct Marketing are not same, the large-scale data limits the learning speed and the customer state is partially observable. Then we proposes corresponding improvement methods, and use the simulation to evaluate these methods. Details as follows:

On the one hand, aiming at the problems that the decision-making time intervals are not same in the Direct Marketing and large-scale data limits the learning speed of model. This research proposes the improved Q-learning algorithms based on the classical Q-learning algorithm. Specifically, a mean normalization method is used to reduce the noise impact on the reward signal caused by the different time intervals among decision points. Then a standardization factor is constructed for Q-learning, and its update method follows the updated method of the value function, thus reducing the deviation of Q value function caused by the unsynchronized update of time interval during the iteration process, thus proposing the Interval-Q algorithm. Besides, for the problem that the training speed of the Interval-Q algorithm is slow when facing to the large-scale data, this research introduces the Temporal Difference TD bias to Q sampling method and proposes the Q sampling method based on the TD bias. Finally, simulation experiments show that the Interval-Q algorithm proposed in this research can achieve higher profits in the irregular Direct Marketing. In addition, the Q sampling method based on TD bias can achieve better results while reducing the number of samples.

On the other hand, aiming at the problems that the traditional Reinforcement Learning algorithm cannot effectively deal with the Partially Observable of the customer state in the Direct Marketing. This research studies the DQN (Deep Q Network) model and proposes the improved DQN model based on two networks. Specifically, firstly, considering the temporality of marketing, the problem mentioned above is solved by using the DQN model based on the RNN network (termed as DQN_RNN) to learn the hidden state. Then, the research points out that it is a challenge for DQN_RNN model to learn the hidden state and approximate the value function in the optimization process of only a network. Therefore, the DQN model based on two networks is proposed: the hidden state is learned from the supervision data through the RNN. After that, the hidden state of the output of the RNN network is used as the input state of the DQN network for Reinforcement Learning. In this way, the advantages of the two networks can be fully utilized, and the hidden state can be better learned while improving the approximation effect of the value function. Finally, in order to obtain a better strategy, this research proposes three improved models by analyzing the network structure and training method. They are separate training model with two networks , one-step joint training model with two networks and two-step joint training model with two networks. Simulation experiments show that the DQN model based on the two networks proposed in this paper can achieve higher returns in the regular Direct Marketing.


\enkeywords{Reinforcement Learning; Value function; Q-learning algorithm; Deep Q Network; Direct Marketing}
\end{enabstract}