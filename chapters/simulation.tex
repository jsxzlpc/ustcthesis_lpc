\chapter{仿真实验}

\section{直邮营销场景仿真分析}
本节，我们选择在直效营销中最经典的直邮营销场景中，研究第三章基于RNN的深度强化学习混合模型及其他的基准模型的应用效果。首先，对实验所使用的数据进行介绍，然后描述按照文献\citep{pednault2002sequential}中的方法构建仿真环境的过程，最后将模型的训练效果在仿真环境中进行测试评估，并且对测试结果进行分析。

\subsection{数据集}

\paragraph{数据集背景}
直邮营销是直效营销中最早的、也是最为经典的应用场景，它是通过对目标客户以邮寄营销信件的方式达到营销宣传的目的。本实验中选择使用UCI数据库中关于直邮营销著名的公开数据集KDD-CUP 1998\footnote{https://kdd.ics.uci.edu/databases/kddcup98/kddcup98.html}作为训练数据，并以此数据集构建仿真环境。需要提到的是，一些关于强化学习技术在直邮营销的研究文献\citep{pednault2002sequential,tkachenko2015autonomous}也都使用了该数据集。

KDD-CUP 1998数据集是由美国非盈利组织PVA（Paralyzed Veterans of America）收集的，该组织通过直邮的方式向潜在捐助者发布捐助活动信息以
筹集资金，为有脊髓损伤疾病的美国退伍军人提供援助。所以，该数据集是由在直邮捐助活动过程中所产生的信息组成，这些信息主要包括个人的基本信息以及两年所进行的23次募捐活动的历史反馈纪录，比如，该是否向该捐助者进行了邮寄、该捐助者是否产生捐助、产生了多少捐助金额等，另外邮寄的时间以及捐助者的回复时间都是可获得的。训练集中包括95412名捐助者在23个捐助活动过程中超过200万次的捐助反馈信息，其中23个捐助活动中又含有11种不同类型的邮件。有关数据集的其它描述信息可在UCI网站上获得。

\paragraph{数据结构}
我们的目标是要为下一个捐助活动提供直邮对象的名单，以及确定应该选择哪一种邮件类型，以使得PVA可获得的累积捐赠额最大化。所以，在KDD-CUP 1998数据集中，我们可供选择的行为有12个，包括11中不同的邮件类型以及一个不发送邮件的行为。因为数据集中包括23个不同时间段的捐助活动，所以，每一个捐助者的捐助历史纪录都可以看作是一个含有23个步骤的时间序列。因此，我们可以用如下长度为67（3*22+1）的序列表示一个捐助者的信息：($o_{1}, a_{1}, r_{1},\cdots,o_{22}, a_{22}, r_{22}, o_{23}$)。其中:

（1）$o_{t}$：捐助者在$t$时刻的可观测信息。主要内容如表~\ref{tab:obser_donors}所示。从表中可以看到，除了age和income特征信息可以直接从数据库中获得以外，其余的可观测特征都可以经过简单的统计运算获得。另外，需要注意的是，后面三个观测特征是基于一个时间周期为六个月的历史窗口总结得到的。所以，在提取特征时，每一条序列数据中前六个月的直邮募捐数据将作为第一个历史窗口，因此，序列数据的长度就变成了49：($o_{1}, a_{1}, r_{1},\cdots,o_{16}, a_{16}, r_{16}, o_{17}$)。
\begin{table}[htbp]
  \centering
  \caption{捐助者的可观测特征}
  \label{tab:obser_donors}
  \begin{tabular}{cl}
    \toprule
      可观察特征 & 描述 \\
    \midrule
      age & 捐助者的年龄 \\
      income & 捐助者的收入 \\
      dontimes & 捐助者捐助的次数 \\
      protimes & 向捐助者邮寄的次数 \\
      frequency & 捐助者捐助的频率：dontimes／protimes\\
      recency & 最近一次捐助距离此时的时间（月） \\
      monetary & 平均的捐助额 \\
      protimes_6 & 最近六个月里该捐助者收到募捐邮件次数 \\
      dontimes_6 & 最近六个月里该捐助者募捐的次数 \\
      montimes_6 & 最近六个月里该捐助者募捐的金额 \\
    \bottomrule
  \end{tabular}
\end{table}

（2）$a_{t}$：上述中提到PVA可采取的12个行为。$\{0,1,2,\cdots,11\}$其中，数字$0$表示不邮寄，其余$1$到$11$数字分别对应一种邮件类型。

（3）$r_{t}$：在执行行为$a_{t}$所能得到的即刻奖赏，对应到数据库中就是捐助者所捐助的金额数减去每条邮寄的成本。

\subsection{仿真环境构建}
在强化学习中，测试、评价模型的好坏需要有稳定的仿真环境。目前，openai提供了许多游戏的仿真环境开源库，科研人员可以通过其提供api接口非常方便的调用这些仿真器，来训练、测试自己模型，所以，这也是目前大多数强化学习的研究者选择在游戏领域进行研究的一个重要方面。然而，在其它的一些应用中，需要研究人员自己根据训练数据构建仿真器。本文中，针对直邮募捐活动，我们按照文献\citep{pednault2002sequential}提供的方法，通过构建关于训练数据集的马尔科夫决策过程对来搭建仿真器，进而评价模型的好坏。

构建的马尔科夫决策过程主要有两个评估模型组成：第一个模型$P(s,a)$是用来预测捐助者的产生反馈的概率，它是一个关于状态特征和行为的函数。第二个模型$A(s,a)$用来预测如果当募捐着对此次募捐邮件产生了反馈，会产生多少募捐额。其中，模型$P(s,a)$使用了基于朴素贝叶斯的树算法进行构建，模型$A(s,a)$使用线性回归树算法尽心构建。

当有了模型$P(s,a)$和$A(s,a)$，我们可以使用如下过程来构建马尔科夫决策过程。

1）给定状态$s$和行为$a$下的即时奖赏$r(s,a)$可以通过这两个模型得到：以$P(s,a)$的概率来掷一枚硬币，并以此来判断捐助者是否会产生反馈。即：出现正面的概率为$P(s,a)$表示捐助者会产生反馈，出现反面的概率为$1-P(s,a)$表示捐助者忽略掉了本次募捐邮件，没有产生任何反馈信息。如果没有出现反馈，那么捐助额为$0$，如果产生了反馈，那么用户的捐助金额为$A(s,a)$。即时奖赏$r(s,a)$等于捐助额减去邮寄的成本。

2）状态转移方程也可以通过使用这两个模型计算每一个观测状态的转移而得到。比如：如果以上述方式掷硬币出现正面朝上，那么观测值dontimes（捐助者捐助的次数）将会加$1$，否则，其值不发生变化。同样的，如果行为$1$到$11$任意一个值出现，那么观测值protimes（向捐助者邮寄的次数）会加$1$，如果行为$0$出现，那么其值保持不变。有了以上这两个值，观测值frequency就可以通过（dontimes／protimes）的方式得到。其它特征也可以通过这种方式更新得到。

有了上述马尔科夫决策过程的公式化定义，我们就可以使用如下的方式来构建我们的评估实验。1）首先选择一定数量规模（10000）的捐助者，并设置他们的初始状态，本实验中将所选择的捐助者的初始状态设置为第23个邮寄活动的状态，即$o_{23}$。2）然后使用强化学习模型的Q值函数来决定对每个捐献者应该采取哪一个行为。3）使用模型$P(s,a)$和$A(s,a)$，我们可以得到预估的即时奖赏以及下一时刻的状态。记录这些得到信息，然后进入下一次的邮寄决策中。就这样，按照上述的过程，每进行一次就会模拟一次虚拟的直邮募捐的过程。

在文献\citep{pednault2002sequential}中作者提出，构建上述仿真环境的假设前提是认为募捐者的交互过程是一个马尔科夫决策过程，然而这不一定是合适的。但是，将模型的环境构建为一个马尔科夫决策过程，然后使用一个强化学习算法从交互中去评价状态行为值函数是一个普遍采用的做法。作者认为，像直复营销这种与人类行为有关的场景都过于复杂，使用简单的MDP是无法较好的捕捉到环境的变化规律的，我们使用这个模拟器去评估我们的算法所产生的策略，只是在真实场景应用前的一个评估实验。所以，这种评估方法是可接受的，也是目前强化学习评估实验所普遍采用的方法。

\subsection{仿真结果及分析}

\paragraph{生命周期价值}

\paragraph{直邮的数量}

\paragraph{每次直邮募捐的利润}

\paragraph{不同数据规模的效果比较}

\section{广告投放场景仿真分析}
本节中，我们以国内某公司一年多来基于某DSP平台进行广告投放的真实数据，训练SVR+Q+MCKP模型，并评估其产生的投放策略效果。首先，介绍了本实验中所使用的数据集背景、经过处理中的数据结构以及各数据结构上各变量的含义和计算方法。然后，在文献\citep{pednault2002sequential}中关于仿真环境构建方法的基础上，实现多渠道广告投放的仿真环境，最后在此仿真器上测试SVR+Q+MCKP模型的学习效果并进行分析。

\subsection{数据集}
\paragraph{数据集背景}
目前，关于广告投放的公开数据集很多，比如Kaggle中的CTR预估数据集\footnote{https://www.kaggle.com/c/avazu-ctr-prediction}、Cirteo Labs的Display广告数据集\footnote{http://labs.criteo.com/category/dataset/}以及在线广告实时竞价数据集\footnote{https://www.kaggle.com/zurfer/rtb}等，但是，这些数据集都是针对的某一个渠道或者某一个渠道中的广告位而言的，而且优化对象是具体的用户。这都与本部分研究场景要求的多渠道、长时间、可追踪用户生命周期价值等特点不符合。本实验所使用数据集来自国内某企业基于某DSP平台进行广告投放的真实数据，该企业进行广告投放的目的是为了获取新用户，进而可以提高企业的营收。

该数据集搜集的时间范围是从2015年10月15日到2017年2月20日，共计494天，其中进行广告投放的时间共475天，其它19天没有进行广告投放。数据来源是同一家DSP平台的四个广告渠道，分别记为渠道A、B、C、D。其中，渠道A的投放数据共454条（天），渠道B数据共460条（天），渠道C数据共443条（天），渠道D数据共464条（天）。DSP商每次（天）反馈的投放效果数据包括：花费额、曝光量、点击量、注册量等信息。

\paragraph{数据结构}
在强化学习的应用中，数据是以序列化的形式存在的<$s_{t}$,$a_{t}$,$r_{t+1}$,$s_{t+1}$>，在进行模型训练前，首先要做的就是从投放数据中找到状态$s_{t}$，行为$a_{t}$以及奖赏$r_{t}$相对应的数据信息。在寻找和确定这些信息时，要充分考虑到应用场景的特点以及所要追求的目标，只有这样实验才会取到较好的效果。下面是本实验中的状态、行为以及奖赏表示信息。

（1）$a_{t}$：花费行为。对应本文第四章中针对每个渠道数据进行离散化后，所产生的离散行为空间中的行为编号。

（2）$r_{t}$：奖赏信息。奖赏信息就是该模型所追求的最大化目标。为了方便比较，在本次实验中，我们分别使用注册量和渠道的ltv值作为奖赏进行试验分析。企业进行广告投放的基本目标是为了获取新用户，所以可以用注册量来作为每次广告投放的奖赏值。但是，企业获取新用户的最终目的是为了增加企业的收入，不能仅仅使用注册量作为广告投放效果的评价指标，因为不同渠道获得的用户质量是不一样，渠道价格也是不一样的。有的渠道获取的用户可能对企业来说价值很大，因为该渠道产生的用户后续会发生产生很多次的交易，那么该渠道的质量就越好，相应的每次投放的价格就越高；而有的渠道虽然可以以很低的成本获取大量的用户，但是来自该渠道的用户后续产生的价值不大，那么该渠道的质量就不高。所以，我们最终应该以渠道的ltv作为此次投放的奖赏值。渠道ltv的具体计算方法是：追踪该渠道带来的每个新用户在未来一个月内所产生的利润之和。所以，渠道的ltv值会有一个月的延迟性，我们不能及时得到本次投放所产生的ltv值，需要我们利用之前的数据进行预测。

（3）$s_{t}$。渠道状态值。为了能较准确的捕捉渠道特征，参考上一个试验中关于状态特征的定义，我们在此基础上设计了更多的状态特征，如表~\ref{tab:obser_ad_ltv}所示当ltv作为奖赏值的状态特征，表~\ref{tab:obser_ad_register}所示为当注册量作为奖赏时的状态特征。其中，festival特征，表示距离法定节假日或重大营销节日（双11和618等节日）的天数，法定节假日提前三天开始算，重大营销节日提前10天开始计算。即：如果该投放时刻距离某法定节假日还有三天，那么此时该特征值为3，如果处在法定节假日中，那么该特征为0，其它时间以此类推，如果不属于以上所有应该标记的时刻，那么此特征值为-1。表~\ref{tab:obser_ad_ltv}中最后一个月内的ltv值因为暂时无法求得，所以为预测的ltv值，roi值等于ltv值减去对应的花费后再除以花费。在表~\ref{tab:obser_ad_register}的roi值等于10000除以注册量，即表示一万元的广告花费会产生多少注册量。
为当ltv作为奖赏值的观测特征
\begin{table}[htbp]
  \centering
  \caption{当ltv作为奖赏值时的状态特征}
  \label{tab:obser_ad_ltv}
  \begin{tabular}{cl}
    \toprule
      状态特征 & 描述 \\
    \midrule
      festival & 距离节日的时间（日） \\
      week & 星期几 \\
      adtimes & 到目前为止该渠道已经投放的次数 \\
      adcost & 到目前为止该渠道投放所花费的金额 \\
      adltv & 到目前为止该渠道投放收获的ltv价值总和 \\
      ltvroi & 到目前为止该渠道基于ltv的roi值 \\
      cost_i & 前i天在渠道所花费的金额，$i=1,2,\cdots,10$ \\
      register_i & 前i天在渠道所产生的注册量 $i=1,2,\cdots,10$\\
      % explore_i & 前i天在渠道所产生的曝光量 $i=1,2,\cdots,10$\\
      % click_i & 前i天在渠道所产生的点击量 $i=1,2,\cdots,10$\\
      ltv_i & 前i天在渠道所产生的ltv值 $i=1,2,\cdots,10$\\
      roi_i & 前i天在渠道所产生的roi值 $i=1,2,\cdots,10$\\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{当注册量作为奖赏值时的状态特征}
  \label{tab:obser_ad_register}
  \begin{tabular}{cl}
    \toprule
      状态特征 & 描述 \\
    \midrule
      festival & 距离节日的时间（日） \\
      week & 星期几 \\
      adtimes & 到目前为止该渠道已经投放的次数 \\
      adcost & 到目前为止高渠道投放所花费的金额 \\
      registerroi & 到目前为止该渠道基于注册量的roi值 \\
      cost_i & 前i天在渠道上所花费的金额，$i=1,2,\cdots,10$ \\
      % exposure_i & 前i天在渠道上所产生的曝光量 $i=1,2,\cdots,10$\\
      % click_i & 前i天在渠道上所产生的点击量 $i=1,2,\cdots,10$\\      
      register_i & 前i天在渠道上所产生的注册量 $i=1,2,\cdots,10$\\
      roi_i & 前i天在渠道上锁产生的基于注册量的roi值 $i=1,2,\cdots,10$\\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{仿真环境构建}
参考文献\citep{pednault2002sequential}中仿真环境的构建方法，我们使用如下方式构建了多渠道投放的仿真环境。

首先需要对每个渠道建立两个评估模型：$A_{reg}(s,a)$和$A_{ltv}(s,a)$，第一个模型$A_{reg}(s,a)$是用来预测在状态$s$下，执行行为$a$时，产生的注册量，第二个模型$A_{ltv}(s,a)$用来预测在状态$s$下，执行行为$a$后，产生的ltv值。两个模型都是基于历史数据使用xgboost算法进行预测。

当构建好评估模型$A_{reg}(s,a)$和$A_{ltv}(s,a)$后，就可以构建关于各渠道的马尔科夫决策过程。1）给定状态s和行为a下的计时奖赏可以通过$A_{reg}(s,a)$或$A_{ltv}(s,a)$得到。2）状态转移方程可以根据采取行为后得到的反馈进行简单的计算后得到。比如：在表~\ref{tab:obser_ad_register}中，
如果在该渠道上进行了投放，那么adtimes值就会加1，根据投放的金额，adcost的值也会进行相应的增长。其它状态特征的转换以此类推。

有了上述马尔科夫决策过程的公式还定义，我们就可以使用如下方式来评估模型的效果。首先，将四个渠道进行状态初始化，从投放历史中任一挑选其中一天数据的状态作为四个渠道的初始状态。然后，使用SVR+Q+MCKP模型的值函数来选择每个渠道应该进行的投放行为（花费金额）。接着，利用$A_{reg}(s,a)$和$A_{ltv}(s,a)$就会得到预估的即时奖赏，并计算下一时刻的状态。最后，记录这些信息，然后进入下一次的投放策略决策过程中。按照这种方式，每进行一次就会模拟一次虚拟的广告投放的过程，进而达到评估策略的目的。

就像文献\citep{pednault2002sequential}作者所说的那样，对现实应用场景假设服从马尔科夫决策过程是不合适，因为简单的马尔科夫决策过程很难能够拟合复杂的现实应用比那话规律。但是，我们使用这种仿真方法仅仅是在真实应用前的评估实验，主要是为了考察模型在这种理想情况下的优劣，所以这种评估方法也是目前被广泛接受并采纳的方法。

\subsection{仿真结果及分析}

\section{本章小结}

 