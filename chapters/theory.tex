\chapter{相关理论知识}
强化学习主要解决序贯决策问题：即需要连续不断地做出决策，才能实现最终的目标，而它的目标是求解一个可以使得回报期望最大化的最优策略。一般的序贯决策问题可以利用马尔科夫决策过程（Markov Decision Process, MDP）的框架来描述，在利用马尔科夫决策过程将问题形式化后，可以使用基于模型的动态规划方法和基于无模型的强化学习方法来解决MDP问题，其中，值函数的估计的方法又包括参数化函数逼近和非参数化函数逼近两种。本章会依次对它们进行简要的介绍。

\section{马尔科夫决策过程和强化学习}
% 介绍马尔科夫决策过程，策略、累计回报、值函数、贝尔曼方程、最优值函数
\paragraph{马尔科夫决策过程}
马尔科夫决策过程是强化学习的理论基础，通常使用一个四元组$<S,A,p,r>$表示，其中：$S$表示有限的状态空间，定义为一个有穷集合$\{ S_1,S_2,\cdots ,S_N \}$，$N$为状态空间的大小。$A$表示有限的行为空间，定义为一个有穷集合$\{ A_1,A_2,\cdots ,A_M \}$，$M$为行为空间的大小。$p$表示状态转移概率，$r$表示奖赏函数。

在任意的离散时间点$t$时，环境状态为$S_{t}$，此刻若系统采取行为$A_t$，则会得到有限的即时奖赏$R_{t}$，并且状态也会转移到下一个状态$S_{t+1}$。奖赏$R_{t}$是根据奖赏函数$r$得到的，下一时刻的状态$S_{t+1}$是根据状态转移概率$p$得到的，又因为马尔科夫决策过程的状态具有马尔科夫性，因此状态转移函数和奖赏函数都仅和当前状态和行为有关，与历史的状态序列和行为序列无关。即：$p(s^{'},r|s,a) = Pr\{S_{t}=s^{'}, R_t=r|S_{t-1}=s, A_{t-1}=a\}$且$\sum_{s^{'}\in S}p(s^{'}|s,a)=1$，$\forall s\in S,\forall a\in A$。根据状态转移的情况，可以分为确定性状态转移和随机性状态转移。确定性状态转移中，会转移到确定性的下一状态，而在随机性状态转移中，迁移到的下一状态是不确定的，是一个随机变量。

\paragraph{策略和回报}
% \subparagraph{策略}
强化学习的目标是从给定马尔科夫决策过程中，寻找最优的策略。所谓策略是指在某一状态下，所采取的动作或所采取动作的概率，通常用符号$\pi$表示：$\pi(s,a)=p[A_{t}=a|S_{t}=s]$。策略也分为确定性策略和随机性策略，其中确定性策略输出的是一个动作序列，而随机性策略输出的是该状态下所采取动作的概率。

% \subparagraph{回报}
当给定一个策略$\pi$后，如何进行策略的评估呢？即用到了累积奖赏（cumulative reward）即回报（return）的概念。假设在时刻$t$后接收到的奖赏序列为$\{R_{t+1}, R_{t+2},\cdots\}$，通常采用折扣累积回报的方式计算回报，那么：
\begin{equation}
G_{t}=\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}
\end{equation}
式中，$\gamma<1$，为一个常量，称为折扣因子。在Agent与环境的交互过程中，如果任务可以自然地被分为带有终止时间片的片段，则该任务称为情节式（episodes）任务。如果任务无法分解成若干片段，整个任务需要不断的进行下去，则改任务称为连续式（continous）任务。

\paragraph{值函数}
我们注意到上一节中的累积回报$G_{t}$是个随机变量，不是一个确定的值，无法作为目标函数进行优化。但是其期望是个确定值，因此可以作为目标函数，并定义为状态值函数，用来评估状态$s$的价值。即：当Agent采取策略$\pi$时，回报服从一个分布，且回报在状态$s$的期望值定义为策略$\pi$的状态值函数：
\begin{equation}
v_{\pi}(s)=\mathbb{E}_{\pi}[G_{t}|S_t=s]=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}|S_t=s]
\end{equation}
相应地，策略$\pi$的状态行为值函数可以用来评估在状态$s$下采取行为$a$的价值，公式化为：
\begin{equation}
\label{seq2_qsa}
q_{\pi}(s,a)=\mathbb{E}_{\pi}[G_{t}|S_t=s,A_t=a]
=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}|S_t=s,A_t=a]
\end{equation}
因为上述状态值函数的表达形式在实际应用中不方便表达，因此，我们可以进行进一步的推导，得到状态值函数的贝尔曼方程：
\begin{equation}
\label{seq1}
\begin{aligned}
v_{\pi}(s)&=\mathbb{E}_{\pi}[G_{t}|S_t=s]\\
&=\mathbb{E}_{\pi}[R_{t+1}+\gamma G_{t+1}|S_t=s]\\
&=\sum_{a}\pi(a|s)\sum_{s^{'}}\sum_{r^{'}}p(s^{'},r|s,a)[r + \gamma\mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s^{'}]]\\
&=\sum_{a}\pi(a|s)\sum_{s^{'},r}p(s^{'},r|s,a)[r+\gamma v_{\pi}(s^{'})], \forall s \in S
\end{aligned}
\end{equation}

同样地，也可以得到状态行为值函数的贝尔曼方程：
\begin{equation}
\label{seq2}
\begin{aligned}
q_{\pi}(s,a)=\sum_{s^{'},r}p(s^{'},r|s,a)[r+\gamma \sum_{a'}\pi(a'|s') q_{\pi}(s^{'},a^{'})], \forall s \in S, \forall a \in A
\end{aligned}
\end{equation}
计算值函数的目的是为了从数据中学习到最优策略，而每个策略对应着一个状态值函数，那么最优策略自然对应着最优状态值函数。所谓的最优状态值函数$v_{*}(s)$是指所有策略中值最大的值函数，即$v_{*}(s)＝=\max_{\pi}v_{\pi}(s)$，同样地，最优状态行为值函数$q_{*}(s,a)$为在所有策略中值最大的状态行为值函数，即$q_{*}(s,a)=\max_{\pi}q_{\pi}(s,a)$

因此，我们又可以由式\eqref{seq1}和式\eqref{seq2}进行推导（因为篇幅限制，推导过程略），分别得到最优状态值函数和最优状态行为值函数的贝尔曼最优方程：
\begin{equation}
\begin{aligned}
v_{*}(s)=\max_{a}\sum_{s^{'},r}p(s^{'},r|s,a)[r+\gamma v_{\pi}(s^{'})]
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
q_{*}(s,a)=\sum_{s^{'},r}p(s^{'},r|s,a)[r+\gamma \max_{a'} {\sum_{a'}\pi(a_{'}|s_{'})} q_{\pi}(s^{'},a^{'})]
\end{aligned}
\end{equation}
最后，若求出了最优状态行为值函数，最优策略可以通过直接最大化 $q_{*}(s,a)$来决定：
\begin{equation}
\begin{aligned}
\pi_{*}(a|s) = 
    \begin{cases}
        1 & if \ a=\argmax_{a\in A}q^{*}(s,a),\\
        0 & otherwise.
    \end{cases}
\end{aligned}
\end{equation}

\section{强化学习经典算法}
根据是否依赖于环境，强化学习可以分为基于模型的动态规划法和模型无关的强化学习算法。在基于模型的动态规划法中，需要提供精确的环境模型而不存在模型学习的过程，然后根据环境的动态性利用贝尔曼公式迭代的求解状态值函数或者状态行为值函数，其本质是动态规划（Dynamic Programming, DP）的思想，通过不断迭代直至稳定，得到精确解。模型无关的强化学习算法无需提供环境模型，根据agent与环境的不断交互收集样本，然后直接利用样本求解状态值函数和状态行为值函数。主要包括蒙特卡罗（Monte Carlo, MC）法和时间差分（Temporal Difference, TD）等算法。

\subsection{动态规划法}
动态规划法是指在给定模型的情况下，计算MDP最优策略的方法。主要包括策略迭代和值迭代两种方法。

策略迭代算法包括策略评估和策略改善两个步骤，且两个步骤交替进行。在策略评估中，算法的每次迭代是建立在上一轮策略改善的基础上，对状态空间中的每个状态进行扫描，并利用贝尔曼公式进行更新，经过不断的迭代，值函数最终收敛至不动点。在策略改善中，算法利用上一轮策略评估得到的值函数，以贪心的方式生成一个新的策略。两
个过程不断交替，直至策略收敛至最终策略。值得一提的是，在贝尔曼公式$\eqref{seq1}$中状态的值函数$v_{\pi}(s)$可以利用其后继状态的值函数$v_{\pi}(s^{'})$来表示，这是利用了自举（bootstrapping）的思想。

以行为状态值函数的策略迭代过程为例，：
\begin{displaymath}
\begin{aligned}
\pi_{0}\xrightarrow{E}q_{\pi_0}\xrightarrow{I}\pi_{1}\xrightarrow{E}q_{\pi_1}\xrightarrow{I}\pi_{2}\xrightarrow{E}q_{\pi_2}\xrightarrow{I}\pi_{3}\xrightarrow{E} \cdots \xrightarrow{I}\pi_{*}\xrightarrow{E}q_{\pi_{*}}
\end{aligned}
\end{displaymath}
其中$E$代表策略评估过程，$I$代表策略改善过程。在收敛到最优策略时，每一轮的策略都好于前一轮的策略。初始时，策略和值函数都是随机的。

在策略评估时，针对每一个行为状态值，使用贝尔曼公式$\eqref{seq2}$进行更新值函数，直到$q_{k+1}$稳定而结束本轮迭代。
\begin{equation}
\begin{aligned}
q_{k+1}(s,a)=\sum_{s^{'},r}p(s^{'},r|s,a)[r+\gamma \sum_{a'}\pi(a^{'}|s^{'}) q_{k}(s^{'},a^{'})]
\end{aligned}
\end{equation}
在策略改善时，对所有的状态，使用贪婪方法求出新的策略。
\begin{equation}
\begin{aligned}
\pi_{k+1}(s)=\argmax_{a}q_{k+1}(s,a)
\end{aligned}
\end{equation}
当策略$\pi$稳定时，迭代过程结束，即得到最优值函数$q_{*}(s,a)$和最优策略$\pi_{*}$。

值迭代算法是对策略迭代的简化，也包括策略评估和策略改善两个环节。与策略迭代不同的是，它不需要等到在策略评估值函数完全收敛时才进行下一次的迭代，而是对全部的状态行为空间每进行一次扫描（更新）就进行策略改善，加快的收敛速度。对每一个状态行为对，值迭代的更新公式如下：
\begin{equation}
\begin{aligned}
q_{k+1}(s,a)=\sum_{s^{'},r}p(s^{'},r|s,a)[r+\gamma \max_{a^{'}} q_{k}(s^{'},a^{'})]
\end{aligned}
\end{equation}
直到$q_{k+1}$稳定，则算法结束。此时因为值函数已经收敛，直接对值函数使用贪心方法就可以得到最优策略。


\subsection{蒙特卡洛方法}
在解决强化学习问题时，由于动态规划法需要提供完整的环境模型且计算代价太大等问题，限制了其实际应用。当没有模型时，我们可以采用蒙特卡罗方法计算值函数，即用经验平均的思想代替随机变量的思想。其中，经验是指按照该策略做了很多次试验，产生很多情节episode，每一个情节就是一次实验，平均就是平均值，按照求解方法不同又可以分为第一次访问蒙特卡罗方法（英文，文献）和每次访问蒙特卡罗方法（英文，文献）。在与环境的交互过程中，当一个情节结束后，测发生值函数的更新和策略的改善。其中值函数的更新采取可递增均值的方法，公式为：
\begin{equation}
\begin{aligned}
v_{k+1}(s)=v_{k}(s)+ \alpha(G_{t}-v_{k}(s))
\end{aligned}
\end{equation}
其中，$\alpha$为学习率，$G_{t}$为从状态$s$出发至情节结束所获的的累积折扣奖赏。

但是，在动态规划中，为了保证值函数的收敛，算法会扫描状态中间的每个状态。而无模型的方法充分评估值函数的前提是每个状态都可以被访问到，因此蒙特卡罗方法采用了探索性初始化的方法，即在迭代每一个情节时，初始状态都是随机分配的，这样可以保证迭代过程中每个状态行为对都能被选中。

\subsection{时间差分方法}
相比于动态规划法，蒙特卡罗法使用了经验平均，虽然摆脱了对模型的依赖，但是需要等到每个情节结束后才可以进行值函数评估和策略更新，所以学习速度慢，学习效率不高。而动态规划因为使用了bootstrapping，所以可以在情节未结束时就可以根据未来值函数估计当前的值函数。时间差分法综合了两者的优点，融合了蒙特卡罗的采样方法和动态规划法的bootstrapping思想。

在TD(0)算法中是一步更新，即值函数直接根据下一个时间步进行学习，估计下一个状态的期望回报。值函数的更新公式为：
\begin{equation}
\begin{aligned}
v(S_{t})=v(S_{t})+\alpha (R_{t+1}+\gamma v(S_{t+1})-v(S_{t}))
\end{aligned}
\end{equation}
式中，$\alpha$为步长参数，控制学习率。$R_{t+1}+\gamma v(S_{t+1})$称为TD目标，$\delta_{t}=R_{t+1}+\gamma v(S_{t+1})-v(S_{t})$为TD偏差。

根据探索策略（行为策略）和评估策略是否为一个策略，可以将强化学习方法分为同策略（on-policy）和异策略（off-policy）两种方法。同样时间差分方法也包括了同策略的Sarsa方法和异策略的Q-learning方法。Sarsa方法中，行动策略和评估策略都是$\epsilon-greedy$的方法，对应的算法伪代码如$\ref{algo:algorithm_1}$所示。

\begin{algorithm}[htbp]
\small
\SetAlgoLined
\SetKwRepeat{Repeat}{repeat}{until} 
% \KwData{this text}
% \KwResult{how to write algorithm with \LaTeX2e }

初始化$Q(s,a)$，$\forall s \in S$，$\forall a \in A$， 给定参数$\alpha$，$\gamma$\;
\Repeat{所有的$Q(s,a)$收敛}{
初始化其实状态$s$\;
根据$\epsilon-greedy$策略在状态$s$下选择行为$a$\;
\Repeat(（对每个情节的每一步）){$s$是终止状态}{
	根据$\epsilon-greedy$策略在状态$s$下选择行为$a$，得到回报$r$和下一状态$s^{'}$，在状态$s^{'}$根据$\epsilon-greedy$策略得到动作$a^{'}$\;
	$Q(s,a)=Q(s,a)+\alpha[r+\gamma Q(s^{'},a^{'}-Q(s,a))]$\;
	$s=s^{'}$，$a=a^{''}$\;
	}
}
输出最终策略：$\pi(s)=\argmax_{a}Q(s,a)$\;
\caption{Sarsa算法伪代码}
\label{algo:algorithm_1}
\end{algorithm}

与Sarsa方法不同，在Q-learning中，行为策略采用$\epsilon-greedy$策略，而目标策略采用贪婪策略，故又称为异策略Q-learning算法，其伪代码如$\ref{algo:algorithm_2}$所示。
\begin{algorithm}[htbp]
\small
\SetAlgoLined
\SetKwRepeat{Repeat}{repeat}{until} 
% \SetAlgoRefName{algorithm_2}
初始化$Q(s,a)$，$\forall s \in S$，$\forall a \in A$， 给定参数$\alpha$，$\gamma$\;
\Repeat{所有的$Q(s,a)$收敛}{
初始化其实状态$s$\;
根据$\epsilon-greedy$策略在状态$s$下选择行为$a$\;
\Repeat(（对每个情节的每一步）){$s$是终止状态}{
	根据$\epsilon-greedy$策略在状态$s$下选择行为$a$，得到回报$r$和下一状态$s^{'}$\;
	$Q(s,a)=Q(s,a)+\alpha[r+\gamma \max_{a} Q(s^{'},a^{'})-Q(s,a)]$\;
	$s=s^{'}$，$a=a^{''}$\;
	}
}
输出最终策略：$\pi(s)=\argmax_{a}Q(s,a)$\;
\caption{Qlearning算法伪代码}
\label{algo:algorithm_2}
\end{algorithm}

在TD(0)中，更新当前值函数时，只用到了下一步状态值函数。而TD($\lambda$)考虑从未来的多步进行学习，并且采用加权的方法融合这多步的估计值，$\lambda \in [0,1]$决定了向未来观察的时间步长度。即
\begin{equation}
\begin{aligned}
V(S_{t})=V(S_{t})+\alpha (G^{(\lambda)}_{t}-V(S_{t}))
\end{aligned}
\end{equation}
其中，
\begin{displaymath}
\begin{aligned}
G^{(\lambda)}_{t}=(1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G^{n}_{t}\\
\end{aligned}
\end{displaymath}
\begin{displaymath}
\begin{aligned}
G^{n}_{t}=R_{t+1}+\gamma R_{t+1}+ \cdots +\gamma^{n-1} R_{t+n}+\gamma^{n} V(S_{t+n})
\end{aligned}
\end{displaymath}

这就是TD($\lambda$)的前向视角表达形式，这种方式同样需要等到整个实验结束才能计算。后向视角利用增量式的更新方式，不需要等到实验结束就可以更新当前状态的值函数，并且引入了资格迹$E_{t}(s)$的概念，资格迹记录了最近被访问过的状态，也就是说，最近且最频繁被访问到的状态会被赋予最大的“资格”。更新方式为：

首先，计算当前状态的TD偏差:$\delta_{t}=R_{t+1}+\gamma V(S_{t+1})-V(S_{t})$

其次，更新资格迹：$E_{t}(s) = 
    \begin{cases}
        \gamma \lambda E_{t-1} & if s \neq s_{t},\\
        \gamma \lambda E_{t-1}＋1 & if s = s_{t}.
    \end{cases}$

最后，对状态空间中的每个状态$s$，更新值函数：$V(s)=V(s)+\alpha \delta_{t} E_{t}(s)$

综上，时间差分方法不仅不需要环境模型，而且它利用增量在线的机制，实现方式简单有效，同时也保证了策略的实施性，称为强化学习中最核心的算法。

\section{值函数逼近方法}
至此，我们可以归纳出强化学习的基本步骤是先评估值函数，然后再利用值函数改善当前的策略。其中，值函数的评估是关键。在之前所介绍的强化学习中，值函数其实是一个表格，其索引是状态或者状态行为对，值迭代更新实际上就是这张表格的迭代更新，因此，这种强化学习称为表格型强化学习，它有一个前提条件：状态空间和行为空间都是离散的，并且状态空间和行为空间不能太大。然而，现实中的很多问题都具有很大的或者连续的状态（行为）空间，比如围棋有 $10^{170}$  个状态空间，控制直升机飞行需要的是一个连续状态空间。在这种情况下，需要我们利用函数逼近的方法表示值函数。从数学的角度来看，函数逼近方法可以分为参数化函数逼近和非参数化函数逼近，其中参数化函数逼近又分为线性参数逼近和非线性参数逼近。

\subsection{参数化函数逼近}
参数化函数逼近是从参数空间到值函数空间的映射，函数的形式和参数的个数事先由先验知识预设，而且函数的参数是通过关于值函数的样本数据来调整的。对于状态(状态行为)值函数可以由一组参数\bfseries $\theta$ \mdseries $\in \mathbb{R}^{n} $ 来近似，即
\begin{equation}
\label{seq_2_3_1}
\begin{aligned}
\hat{v}(s,\bm{\theta})\approx v_{\pi}(s)
\end{aligned}
\end{equation}
\begin{equation}
\label{seq_2_3_2}
\begin{aligned}
\hat{q}(s,a,\bm{\theta})\approx q_{\pi}(s,a)
\end{aligned}
\end{equation}

从近似的状态行为值函数中，我们可以发现针对每个状态行为对，不再存储其状态行为值，而是只存储一组参数，把从已知的状态行为对学到的函数通用化推广至那些未碰到的状态行为对中，这对于大规模的状态行为空间来说，相当于对其状态行为空间进行了压缩。由于\bfseries $\theta$ \mdseries $\in \mathbb{R}^{n}$，因此所需的存储开销为$O(n)$，当状态和行为空间规模较大且均为离散时，$n$通常远远小于为每个状态动作对存储值的开销$|S|\cdot|A|$。

从表格型值函数的更新过程中，我们可以看出无论是蒙特卡罗方法还是时间差分方法，都是朝着一个目标值更新的，这个目标值在蒙特卡罗方法中是$G_{t}$，在时间差分方法中是$r+\gamma Q(s^{'},a^{'})$，在$TD(\lambda)$中是$G^{\lambda}_{t}$。同样的，将表格型强化学习值函数的更新过程推广到值函数逼近过程，有如下形式：函数逼近$\hat{v}(s,\mathbf{\theta})$实际上是一个监督学习的过程，其数据和标签对为$(S_{t}, U_{t})$，其中$U_{t}$等价于蒙特卡罗方法中的$G_{t}$，时间差分方法中的$r+\gamma Q(s^{'},a^{'})$,以及$TD(\lambda)$中是$G^{\lambda}_{t}$。

可以得到训练的目标函数：通过找到参数向量$\mathbf{\theta}$，最小化近似函数$\hat{v}(s,\mathbf{\theta})$与实际函数$v_{\pi}(s)$的均方差：
\begin{equation}
\begin{aligned}
J(\mathbf{\theta})=\mathbb{E}_{\pi}[(v_{\pi}(s)-\hat{v}(s,\mathbf{\theta}))^2]
\end{aligned}
\end{equation}

求解值函数更新可以分为增量式学习方法和批学习方法。其中，随机下降法是最常用的增量式学习方法。
使用梯度下降法，可以得到值函数的更新如下：
\begin{equation}
\begin{aligned}
\triangle \theta = \alpha (v_{\pi}(s)-\hat{v}(s,\mathbf{\theta})) \triangledown_{\theta} \hat{v}(s,\mathbf{\theta})
\end{aligned}
\end{equation}

所谓批方法是指给定经验数据集$D={<s_{1},v^{\pi}_{1}>,<s_{2},v^{\pi}_{2}>,\cdots,<s_{T},v^{\pi}_{T}>}$，找到最好的拟合函数$\hat{v}(s,\mathbf{\theta})$，使的$LS(\mathbf{\theta})=\sum_{t=1}^{T}(v^{\pi}_{t}-\hat{v}^{T}_{t}(s_{t},\mathbf{\theta}))$ 最小。

参数化函数逼近根据所使用逼近函数的不同，可分为参数化线性函数逼近和参数化非线性函数逼近两种。以状态行为值函数为例，线性函数逼近可以表示为：
\begin{displaymath}
\begin{aligned}
\hat{q}(s,a,\mathbf{\theta})=\sum^{n}_{i=1}\phi_{i}(s,a)\theta_{i}=\mathbf{\phi }(s,a)^{T}\mathbf{\theta }
\end{aligned}
\end{displaymath}
其中，$\mathbf{\theta}=(\theta_{1},\cdots,\theta_{n}\in\mathbb{R})$，$\mathbf{\phi}(s,a)=(\phi_{1}(s,a),\cdots,\phi_{n}(s,a))^{T}$，为状态行为对$(s,a)$的特征向量，$\phi_{i}(s,a)$为特征函数或者称为基函数，常见的基函数有：多项式基函数、傅立叶基函数以及径向基函数等。线性函数逼近不但形式简单、而且可以收敛到全局最优，缺点是表征能力较弱。但是正是由于其具备良好的收敛性和理论简单性，在强化学习的参数化方法中得到了充分应用。

在非线性函数逼近中，函数逼近器是关于参数$\mathbf{\theta}$的非线性函数。常用的非线性函数逼近方法为人工神经网络，关于此方面的详细介绍参见本文第三章。参数化非线性函数逼近的优点是有很强的表征能力和逼近能力，可以对目标函数以任意精度逼近，但是其收敛性难以保证，容易陷入局部最优，使的学术界对其的研究停滞了很长时间，直到DeepMind团队的出现，提出了一些改进的深度神经网络的训练方法，很大程度上解决了神经网络在强化学习问题中的收敛性问题，并且取得了令人振奋的实验表现，引发了针对此问题的新的研究热潮。本文的第三章就是针对深度神经网络进行函数逼近的研究。

\subsection{非参数化函数逼近}
在参数化函数逼近过程中，基函数的形式和参数的个数都需要提前设定，并且值函数的逼近效果很大程度上受到人为的经验的影响。而在非参数化函数逼近中，参数的个数以及基函数的形式并不是固定的，而是由样本决定，具有很高的灵活性。非参数化函数逼近方法包括基于核函数的方法和基于高斯过程的方法。因为本文第四章是使用的基于核函数的非参数化函数逼近的方法，所以此处仅对基于核的非参数函数逼近进行一个简单的介绍。

基于核函数逼近模型可以表示为
\begin{displaymath}
\begin{aligned}
\hat{q}(s,a,\mathbf{\theta})=\sum^{n_{s}}_{l_{s}=1}k((s,a),(s_{l_{s}},a_{l_{s}}))\mathbf{\theta}_{l_{s}}
\end{aligned}
\end{displaymath}
式中，$\mathbf{\theta_{1},\cdots,\theta_{n_{s}}}$为参数向量，${(s_{l_{s}},a_{l_{s}})|l_{s}=1,\cdots,n_{s}}$为样本集合。$k:S\times A \times S \times A \to \mathbb{R} $为核函数。其中，核函数有线性核函数、多项式核函数、径向基核函数以及Sigmoid核函数等。

非参数化函数学习过程中完全依赖于样本，虽然带来一定的逼近灵活性，但是收敛性难以得到保证，

\section{本章小结}
 本章主要介绍了强化学习的理论基础知识。首先介绍了马尔科夫决策过程，然后介绍了强化学习的三种求解方法。并且针对函数逼近问题的框架作了简要描述，为后面章节做了铺垫。