\chapter{相关理论概述}

强化学习主要用于解决序贯决策问题，即需要连续不断的做出决策才能实现最终的目标。而一般的序贯决策问题可以使用马尔科夫决策过程（Markov Decision Process, MDP）的框架来进行描述，在利用MDP将序贯决策问题形式化后，又可以使用基于模型的动态规划方法和基于无模型的强化学习方法来解决该问题。特别地，在强化学习方法中，对于状态和行为空间较大的问题，往往采用值函数逼近的方法进行求解。本章会依次对它们进行简要的介绍。

\section{强化学习概述}
强化学习是机器学习（Meachine Learning, ML）的重要组成部分，它是从控制工程、心理学和运筹学等众多学科交叉发展而来的。强化学习最早可以追溯到十九世纪九十年代的巴普洛夫条件反射实验，但是，直到二十世纪九十年代才被重视起来，并随着强化学习理论的不断发展和完善，目前已经成为人工智能领域最火热的研究方向，广泛应用于机器人控制、无人驾驶和游戏等领域。

强化学习主要用于解决序贯决策问题，所谓决策，是指面对特定的状态，采取什么样的行为，才能使的回报最大化；所谓序贯，是指需要连续不断的做出决策。所以，强化学习属于一种交互式的学习方法，主要是通过智能体（Agent）与环境不断交互，并以最大化未来的累积奖赏作为优化目标，从环境反馈的奖赏信息中不断进行学习。具体交互过程如图$\ref{fig:强化学习过程_}$所示：
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{强化学习过程_}
\caption{强化学习的交互过程}
\label{fig:强化学习过程_}
\end{figure}

（1）Agent感知当前的环境状态（state）；

（2）根据当前的状态和奖赏值（reward，强化信号），Agent选择一个行为（action）并执行；

（3）当Agent所选择的行为作用于环境时，环境变会转移到下一个新的状态的同时并反馈出新的奖赏信息；

（4）Agent根据环境反馈的奖赏，计算出回报值（return），并将该回报值作为更新内部策略的依据。

 在这一过程中，Agent并不会被告知应该采取哪个行为，而只能经过不断的尝试后，根据得到的环境反馈信息做出判断。Agent选择行为的原则是：所选择的行为应该在以后的学习过程受到环境奖励的概率增大，受到惩罚（负奖赏）的概率减小，也就是说，当前采取的行为只与最终的目标有关。因为在序贯决策中，奖励信号是有延迟的，所以Agent有时候宁愿会牺牲即时（短期）的奖励以获取更多的长期奖励。从以上分析中可以看出，试错搜索和延迟回报是强化学习中最显著的特征。

 与机器学习中的监督学习方法不同，强化学习在学习过程中并不需要多样化的标签数据，而只需要带有奖赏的交互数据。另外，监督学习是将模型预测的输出值与样本标签之间所产生的误差信号，反馈给模型进而指导模型的学习，而在强化学习中，由环境产生的奖赏信号是对所产生行为好坏的一种评价，Agent在行为与评价中获取知识，不断的改进行为方案以适应环境，并不是指导Agent如何去产生正确的动作。

 根据学习过程中优化对象的不同，可以将强化学习方法分为基于值函数的方法和基于策略搜索的方法两类。在基于值函数的方法中，通过迭代计算值函数，再根据值函数改善策略；而在策略搜索的方法中，直接对策略进行迭代。特别地，本文只针对基于值函数的强化学习方法进行研究。

\section{强化学习的组成要素}
因为强化学习是建立在马尔科夫决策过程的基础上的，所以本节首先对马尔科夫决策过程进行介绍，接着对强化学习中的重要组成要素：策略、回报和值函数等概念进行详细阐述，以便后续展开对强化学习方法的讨论。

% 介绍马尔科夫决策过程，策略、累计回报、值函数、贝尔曼方程、最优值函数
\paragraph{马尔科夫决策过程}
强化学习可以使用马尔科夫决策过程框架进行形式化的描述，进一步地，如果状态和行为都是有限的，则称之为有限马尔科夫决策过程（finite Markov Decision Process, finite MDP），本章的所介绍的MDP无特别说明，均指的是finite MDP。finite MDP通常表示为一个四元组$<S,A,p,r>$的形式，其中：$S$表示有限的状态空间，定义为一个有穷集合$\{ S_1,S_2,\cdots ,S_N \}$，$N$为状态空间的大小。$A$表示有限的行为空间，定义为一个有穷集合$\{ A_1,A_2,\cdots ,A_M \}$，$M$为行为空间的大小。$p$表示状态转移函数，$r$表示奖赏函数。

考虑在MDP中任意的离散时间点$t$时，环境的状态为$S_{t}$，此刻Agent若采取行为$A_{t}$，便会得到有限的即时奖赏$R_{t}$，并且环境状态也会相应的发生改变，转移到下一状态$S_{t+1}$。其中，下一时刻的状态$S_{t+1}$是根据状态转移函数$p$确定的，奖赏$R_{t}$是根据奖赏函数$r$求得的。

另外，根据转移方式的不同，又可以分为确定性环境转移和随机性环境转移。其中，在确定性环境转移中，状态转移到的下一状态是确定性的，其奖赏函数只和当前状态和所采取的行为有关，是对即时奖赏的评价。而在随机性状态转移中，状态转移到的下一状态是不确定的，而是一个随机变量，并且奖赏函数除了和当前状态和所采取的行为有关之外，还依赖于下一个状态，即是对长期效果的评价。

% 如式\eqref{seq:p}和\eqref{seq:rr}：
% \begin{equation}\label{seq:p}
% p(s^{'}|s,a)=\bm{\text{Pr}}\{S_{t+1}=s|S_{t}=s,A_{t}=a\}
% \end{equation}
% \begin{equation}\label{seq:rr}
% r(s,a,s^{'})=\mathbb{E}[R_{t}|S_{t}=s,A_{t}=a,S_{t+1}=s^{'}]
% \end{equation}

% 其中，公式\eqref{seq:p}表示在给定状态s时，执行行为a后，环境状态转移到s^{'}的概率。公式\eqref{seq:rr}表示当前状态s和行为a时，环境转移到下一状态s^{'}所产生的期望奖赏。




% 又因为马尔科夫决策过程的状态具有马尔科夫性，因此状态转移函数和奖赏函数都仅和当前状态和行为有关，与历史的状态序列和行为序列无关。即：$p(s^{'},r|s,a) = Pr\{S_{t}=s^{'}, R_t=r|S_{t-1}=s, A_{t-1}=a\}$且$\sum_{s^{'}\in S}p(s^{'}|s,a)=1$，$\forall s\in S,\forall a\in A$。
\paragraph{策略和回报}
% \subparagraph{策略}
% 强化学习的目标是从给定的马尔科夫决策过程中，寻找最优的策略。
所谓策略，是指从状态$s$到行为$a$的映射，通常用符号$\pi$表示。
% 指在某一状态下，所采取的动作或所采取动作的概率，。
强化学习中的策略分为确定性策略和随机性策略，其中，确定性策略定义为：$\pi$： $S \to A$ ，其输出的是一个动作序列，而随机性策略定义为：$\pi$： $S \times A \in [0,1]$ ，其输出的是该状态下所采取动作的概率：$\pi(s,a)=p[A_{t}=a|S_{t}=s]$。

% \subparagraph{回报}
强化学习的任务是从与环境的不断交互中，学习一个策略$\pi$，使其产生的累积奖赏（cumulative reward）最大化，即回报最大化。
% 当给定一个策略$\pi$时，如何进行策略的评估呢？此时便用到了累积奖赏的概念。
假设在时刻$t$后接收到的奖赏序列为$\{R_{t}, R_{t+1}, R_{t+2},\cdots\}$，那么采用折扣累积奖赏的方式计算回报可以表示为：
\begin{equation}\label{seq:reward}
G_{t}=\sum_{k=0}^{\infty}\gamma^{k}R_{t+k}
\end{equation}

式中，$G_{t}$为回报，$\gamma<1$，为一个常量，称为折扣因子。特别地，在系统与环境的交互过程中，如果任务可以自然地被分为带有终止时间片的片段，则该任务称为情节式（episodes）任务。如果任务无法分解成若干片段，整个任务需要不断的进行下去，则改任务称为连续式（continous）任务。

\paragraph{值函数}

计算状态值函数的目的是为了构建学习算法，从数据中学到最优策略。在Agent的学习过程中，当给定策略$\pi$时，考虑如何评价某一状态的价值。因为强化学习的目标是追求累积奖赏最大化，所以很自然的想法是可以利用上述回报的计算方式来衡量某一状态的价值。但是，由于某一状态下的产生的状态序列可以会有很多种，所以回报$G_{t}$是个随机变量，不是一个确定的值，无法作为目标函数进行优化。然而，其期望是个确定值，因此可以作为状态值函数的定义：当Agent采取策略$\pi$时，回报服从一个分布，将状态$s$的回报期望值定义为状态$s$的值函数。公式化为\eqref{seq2_vs}：
\begin{equation}
\label{seq2_vs}
v_{\pi}(s)=\mathbb{E}_{\pi}[G_{t}|S_t=s]=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k}|S_t=s]
\end{equation}

% Q函数是另一种评估值函数的方法。Watkins指出：。

在某些时候，记录状态行为对的值比只记录状态的值更有用。所以，考虑在状态$s$下采取行为$a$后所产生回报的期望值，并将其定义为状态行为值函数，公式化为\eqref{seq2_qsa}：
\begin{equation}
\label{seq2_qsa}
q_{\pi}(s,a)=\mathbb{E}_{\pi}[G_{t}|S_t=s,A_t=a]
=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k}|S_t=s,A_t=a]
\end{equation}

上述状态值函数\eqref{seq2_vs}的表达形式在实际计算和编程的时候很不方便，因此，可以经过进一步的推导，得到状态值函数的贝尔曼方程：
\begin{equation}
\label{seq1}
\begin{aligned}
v_{\pi}(s)&=\mathbb{E}_{\pi}[G_{t}|S_t=s]\\
&=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k} | S_t=s]\\
&=\mathbb{E}_{\pi}[R_{t} + \gamma \sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}|S_t=s]\\
&=\sum_{a}\pi(a|s)\sum_{s^{'}}p(s^{'}|s,a)[r(s,a,s^{'}) + \gamma \mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^{k} R_{t+k+1} |S_{t+1}=s^{'}]]\\
&=\sum_{a}\pi(a|s)\sum_{s^{'}}p(s^{'}|s,a)[r(s,a,s^{'})+\gamma v_{\pi}(s^{'})], \forall s \in S
\end{aligned}
\end{equation}

其中，$p(s^{'}|s,a) = \bm{\text{P}}\{S_{t+1}=s^{'}|S_{t}=s, A_{t}=a\}$，表示在状态$s$时执行行为$a$后，环境转移到下一个状态$s_{'}$的概率。$r(s,a,s^{'})=\mathbb{E}[R_{t}|S_{t}=s,A_{t}=a,S_{t+1}=s^{'}]
$，表示在给定当前状态$s$和行为$a$时，环境转移到下一状态$s^{'}$所产生的期望奖赏。

同样地，也可以得到状态行为值函数的贝尔曼方程：
\begin{equation}
\label{seq2}
\begin{aligned}
q_{\pi}(s,a)=\sum_{s^{'}}p(s^{'}|s,a)[r(s,a,s^{'})+ \gamma \sum_{a'}\pi(a'|s') q_{\pi}(s^{'},a^{'})], \forall s \in S, \forall a \in A
\end{aligned}
\end{equation}

如上文所述，计算值函数的目的是为了构建学习算法，并从数据中学习到最优策略，而每个策略对应着一个状态值函数，那么最优策略自然对应着最优状态值函数。所谓的最优状态值函数$v_{*}(s)$是指所有策略中值最大的值函数，即$v_{*}(s)＝=\max_{\pi}v_{\pi}(s)$，同样地，最优状态行为值函数$q_{*}(s,a)$为在所有策略中值最大的状态行为值函数，即$q_{*}(s,a)=\max_{\pi}q_{\pi}(s,a)$

因此，由式\eqref{seq1}和式\eqref{seq2}进行推导，又可以分别得到最优状态值函数$v_{*}(s)$和最优状态行为值函数$q_{*}(s,a)$的贝尔曼最优方程（因为篇幅限制，推导过程略）：
\begin{equation}
\begin{aligned}
v_{*}(s)=\max_{a}\sum_{s^{'}}p(s^{'}|s,a)[r(s,a,s^{'})+\gamma v_{*}(s^{'})]
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
q_{*}(s,a)=\sum_{s^{'}}p(s^{'}|s,a)[r(s,a,s^{'}) +\gamma \max_{a'} q_{*}(s^{'},a^{'})]
\end{aligned}
\end{equation}

最后，若求出了最优状态行为值函数，最优策略可以通过直接最大化 $q_{*}(s,a)$来决定：
\begin{equation}
\begin{aligned}
\pi_{*}(a|s) = 
    \begin{cases}
        1 & if \ a=\argmax_{a\in A}{q^{*}(s,a)},\\
        0 & otherwise.
    \end{cases}
\end{aligned}
\end{equation}

最后，需要强调的是：从公式\eqref{seq2_vs}和公式\eqref{seq2_qsa}可以看出，值函数是对未来奖赏（回报）的一种预测，对于一个状态$s$，如果它获得的即时（短期）奖赏比较低，并不代表它的状态值就低，因为如果在状态$s$随后的状态里产生了很高的奖赏，仍然会取得比较高的状态值，这就是强化学习在学习过程中可以考虑长期收益最大化的原因，也是本文选择使用强化学习技术应用于营销问题以获取长期收益最大化的原因。

% 但是，因为值函数的计算需要考虑到其后续状态的情况，所以
% 求解值函数需要Agent在其整个生命周期内通过一系列的观察和不断的估计才能得到。
所以，接下来的工作就是如何快速而准确地进行最优值函数的求解，进而求得最优的策略。

\section{基于值函数的强化学习算法}
根据是否依赖于环境，值函数的求解方法可以分为基于模型的动态规划法和模型无关的强化学习算法。在基于模型的动态规划法中，需要提供精确的环境模型，然后根据环境的动态性利用贝尔曼方程迭代的求解值函数，其本质是动态规划（Dynamic Programming, DP）的思想，通过不断迭代直至稳定，得到精确解。模型无关的强化学习算法无需提供环境模型，从Agent与环境的不断交互中收集样本，然后直接利用样本求解状态值函数和状态行为值函数。模型无关的强化学习算法主要包括蒙特卡罗（Monte Carlo, MC）法和时间差分（Temporal Difference, TD）等算法。

\subsection{动态规划法}
动态规划法是在给定模型的情况下，求解值函数和最优策略的方法，它是研究强化学习方法的理论基础。根据迭代方式的不同，动态规划法又可分为策略迭代和值迭代两种方法。
\paragraph{策略迭代}

策略迭代法主要包括策略评估和策略改善两个步骤，并且这两个步骤在学习的过程中依次交替进行。具体地，当进行策略评估的时候，算法每轮都需要进行多次迭代，并且在每次迭代时，需要根据上一轮改善的策略对状态空间里的每个状态进行扫描，然后使用贝尔曼方程进行状态值的更新，经过不断的迭代后，使值函数最终收敛至不动点，进入策略改善步骤。在进行策略改善的时候，算法利用上一轮策略评估得到的值函数，以贪心的方式生成一个新的策略。就这样，不断循环以上两个步骤，直至策略收敛至最优策略。特别地，在进行策略评估的时候，贝尔曼方程$\eqref{seq1}$中状态值$v_{\pi}(s)$的计算因为使用了其后继状态的值函数$v_{\pi}(s^{'})$来表示，所以，用到了自举（bootstrapping）的思想。

下面，以状态行为值函数的策略迭代过程为例，进行详细的介绍：
\begin{displaymath}
\begin{aligned}
\pi_{0}\xrightarrow{E}q_{\pi_0}\xrightarrow{I}\pi_{1}\xrightarrow{E}q_{\pi_1}\xrightarrow{I}\pi_{2}\xrightarrow{E}q_{\pi_2}\xrightarrow{I}\pi_{3}\xrightarrow{E} \cdots \xrightarrow{I}\pi_{*}\xrightarrow{E}q_{\pi_{*}}
\end{aligned}
\end{displaymath}

其中，$E$代表策略评估过程，$I$代表策略改善过程。初始时，策略和值函数都是随机的。在策略评估时，针对每一个状态行为值，使用贝尔曼方程进行值函数的更新，直到$q_{k+1}$稳定便结束本轮迭代，如式\eqref{seq:beierman2}所示，其中$k$表示本轮迭代的次数。
\begin{equation}\label{seq:beierman2}
\begin{aligned}
q_{k+1}(s,a)=\sum_{s^{'}}p(s^{'}|s,a)[r(s,a,s^{'})+ \gamma \sum_{a'}\pi(a'|s') q_{k}(s^{'},a^{'})]
% q_{k+1}(s,a)=\sum_{s^{'},r}p(s^{'},r|s,a)[r+\gamma \sum_{a'}\pi(a^{'}|s^{'}) q_{k}(s^{'},a^{'})]
\end{aligned}
\end{equation}

在策略改善时，对每一个状态，使用贪婪策略对当前策略进行改善，如\eqref{seq:beierman3}所示，其中$l$表示策略改善的轮数。
\begin{equation}\label{seq:beierman3}
\begin{aligned}
\pi_{l+1}(s)=\argmax_{a}q_{l+1}(s,a)
\end{aligned}
\end{equation}

在收敛到最优策略之前，每一轮的策略都好于前一轮的策略。当策略$\pi$稳定时，动态规划过程结束，即得到最优值函数$q_{*}(s,a)$和最优策略$\pi_{*}$。
\paragraph{值迭代}

值迭代算法是对策略迭代算法过程的简化，同样也包括策略评估和策略改善两个环节。与策略迭代不同的是，在进行策略改善的时候，它不需要等到值函数完全收敛时才进行改善，而是对全部的状态行为空间每进行一次扫描（更新）就进行一次策略改善，通过这种方式，加快了值函数的收敛速度。对每一个状态行为对，值迭代的更新公式如式\eqref{seq:beierman4}：
\begin{equation}
\begin{aligned}\label{seq:beierman4}
q_{l+1}(s,a)=\sum_{s^{'}}p(s^{'}|s,a)[r(s,a,s^{'})+ \gamma \max_{a^{'}} q_{l}(s^{'},a^{'})]
% q_{k+1}(s,a)=\sum_{s^{'},r}p(s^{'},r|s,a)[r+\gamma \max_{a^{'}} q_{k}(s^{'},a^{'})]
\end{aligned}
\end{equation}

直到$q_{l+1}$稳定，算法结束。此时因为值函数已经收敛，直接对值函数使用贪心方法就可以得到最优策略。

\subsection{蒙特卡洛方法}
由公式\eqref{seq:beierman2}和公式\eqref{seq:beierman3}可以看出，动态规划法在求解值函数时需要提供完整的环境模型（状态转移概率）且计算代价太大，限制了其在实际场景中的应用。于是出现了模型无关（Model-free）的算法：蒙特卡罗方法。

蒙特卡罗方法采用了经验平均的思想来代替随机变量的思想，因而不再需要提供环境模型。其中，经验是指按照该策略做了很多次试验，产生很多情节（episode），每一个情节就是一次实验，平均就是指的平均值，按照求解方法不同又可以分为第一次访问（First-visit）蒙特卡罗方法和每次访问（Every-visit）蒙特卡罗方法。在与环境的交互过程中，当一个情节结束后，MC变利用情节中的数据进行值函数的更新和策略的改善。另外，值函数的更新可使用可递增（Incremental ）计算均值的方法，以状态值函数为例，更新公式为\eqref{seq:mc}：
\begin{equation}\label{seq:mc}
\begin{aligned}
V_{k+1}(S_{t})=V_{k}(S_{t})+ \alpha(G_{t}-V_{k}(S_{t}))
\end{aligned}
\end{equation}

其中，$V_{k+1}(S_{t})$表示第$k+1$次迭代时$v_{k+1}(S_{t})$估计值，$\alpha$为学习率，$G_{t}$为从状态$s$出发至情节结束所获的的累积折扣奖赏。

在动态规划中，为了保证值函数的收敛，算法会扫描状态空间的每一个状态。而蒙特卡罗方法想要充分评估值函数的前提是保证每个状态都可以被访问到，因此MC采用了探索性初始化（Exploring Start）的方法，即在每一个情节在迭代时，其初始状态都是随机分配的。

\subsection{时间差分方法}
相比于动态规划法，蒙特卡罗法使用了经验平均，虽然摆脱了对模型的依赖，但是必须要等到每个情节结束后才可以进行值函数评估和策略更新，所以学习速度慢，学习效率不高。而动态规划因为使用了bootstrapping思想，可以在情节未结束时就可以根据未来值函数估计当前的值函数。时间差分法综合了两者的优点，融合了蒙特卡罗的采样方法和动态规划的bootstrapping思想。

所谓时间差分是指对同一事件或变量在连续两个时刻观测的偏差。时间差分算法是指一步更新算法，即值函数直接根据下一个时间步进行学习，估计下一个状态的值函数。假设在状态$S_{t}$时，Agent采取行为$A_{t}$，获得奖赏$R_{t}$的同时状态转移到$S_{t+1}$。
% 因为环境从状态$S_{t}$向$S_{t+1}$以外的其它状态转移的概率为0，设$V(S_{t})$为最优$v_{*}(S_{t})$的估计值，那么状态$S_{t}$的值函数为：
% \begin{equation}
% \begin{aligned}
% V(S_{t})=R_{t+1}+\gamma V(S_{t+1})
% \end{aligned}
% \end{equation}
那么，时刻$t$的时间差分$\delta_{t}$可表示为\eqref{shijiachafen}：
\begin{equation}\label{shijiachafen}
\begin{aligned}
\delta_{t}=R_{t+1}+\gamma V(S_{t+1})-V(S_{t})
\end{aligned}
\end{equation}

值函数的更新公式可表示为\eqref{shijiachafen2}：
\begin{equation}\label{shijiachafen2}
\begin{aligned}
V(S_{t}) \gets V(S_{t})+\alpha (R_{t+1}+\gamma V(S_{t+1})-V(S_{t}))
\end{aligned}
\end{equation}

式中，$\alpha$为步长参数，控制学习率。$R_{t+1}+\gamma V(S_{t+1})$称为TD目标，与式\eqref{seq:mc}中的$G_{t}$相对应，两者不同之处在于TD目标利用了bootstrapping方法估计当前值函数。

另外，根据探索策略（行为策略）和评估策略是否为一个策略，可以将强化学习方法分为同策略（on-policy）和异策略（off-policy）两种方法。同样，时间差分方法也包括了同策略的Sarsa方法和异策略的Q-learning方法。在Sarsa方法中，行动策略和评估策略都是$\epsilon-greedy$的方法，对应的算法伪代码如$\ref{algo:algorithm_1}$所示。其中$\epsilon-greedy$方法是指，当给定一个$0$到$1$之间的数$\epsilon$后，以$\epsilon$的概率随机选择行为，以$1-\epsilon$的概率选择最好的行为：$\pi(S_{t})=\argmax_{a} Q(S_{t},a)$。

\begin{algorithm}[htbp]
\small
\SetAlgoLined
\SetKwRepeat{Repeat}{Repeat}{until} 
% \KwData{this text}
% \KwResult{how to write algorithm with \LaTeX2e }
初始化$Q(s,a)$，$\forall s \in S$，$\forall a \in A$， 给定参数$\alpha$，$\gamma$\;
\Repeat{所有的$Q(s,a)$收敛}{
初始化起始状态$s$\;
根据$\epsilon-greedy$策略在状态$s$下选择行为$a$\;
\Repeat(（对每个情节的每一步）){$s$是终止状态}{
	在状态$s$下选择行为$a$，得到奖赏$r$和下一个状态$s^{'}$\;
	在状态$s^{'}$下根据$\epsilon-greedy$策略得到动作$a^{'}$\;
	$Q(s,a) \gets Q(s,a)+\alpha[r+\gamma Q(s^{'},a^{'}) - Q(s,a)]$\;
	$s=s^{'}$，$a=a^{'}$\;
	}
}
输出最终策略：$\pi(s)=\argmax_{a}Q(s,a)$\;
\caption{Sarsa算法}
\label{algo:algorithm_1}
\end{algorithm}

与Sarsa方法不同，在Q-learning方法中，行为策略采用$\epsilon-greedy$策略，而目标策略采用贪婪策略，故又称为异策略Q-learning算法，其伪代码如$\ref{algo:algorithm_2}$所示。
\begin{algorithm}[htbp]
\small
\SetAlgoLined
\SetKwRepeat{Repeat}{repeat}{until} 
% \SetAlgoRefName{algorithm_2}
初始化$Q(s,a)$，$\forall s \in S$，$\forall a \in A$， 给定参数$\alpha$，$\gamma$\;
\Repeat{所有的$Q(s,a)$收敛}{
初始化起始状态$s$\;
% 根据$\epsilon-greedy$策略在状态$s$下选择行为$a$\;
\Repeat(（对每个情节的每一步）){$s$是终止状态}{
	在Q函数中，根据$\epsilon-greedy$策略在状态$s$下选择行为$a$\;
	执行行为$a$后，得到回报$r$和下一状态$s^{'}$\;
	$Q(s,a) \gets Q(s,a)+\alpha[r+\gamma \max_{a} Q(s^{'},a)-Q(s,a)]$\;
	$s=s^{'}$\;
	}
}
输出最终策略：$\pi(s)=\argmax_{a}Q(s,a)$\;
\caption{Qlearning算法}
\label{algo:algorithm_2}
\end{algorithm}

% 在TD(0)中，更新当前值函数时，只用到了下一步状态值函数。而TD($\lambda$)考虑从未来的多步进行学习，并且采用加权的方法融合这多步的估计值，$\lambda \in [0,1]$决定了向未来观察的时间步长度。即
% \begin{equation}
% \begin{aligned}
% V(S_{t} )\gets V(S_{t})+\alpha (G^{(\lambda)}_{t}-V(S_{t}))
% \end{aligned}
% \end{equation}
% 其中，
% \begin{displaymath}
% \begin{aligned}
% G^{(\lambda)}_{t}=(1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G^{(n)}_{t}\\
% \end{aligned}
% \end{displaymath}
% \begin{displaymath}
% \begin{aligned}
% G^{(n)}_{t}=R_{t+1}+\gamma R_{t+1}+ \cdots +\gamma^{n-1} R_{t+n}+\gamma^{n} V(S_{t+n})
% \end{aligned}
% \end{displaymath}

% 这就是TD($\lambda$)的前向视角表达形式，但是，这种方式需要等到整个实验结束才能计算。后向视角利用增量式的更新方式，不需要等到实验结束就可以更新当前状态的值函数，并且引入了资格迹$Z_{t}(s)$的概念，资格迹记录了最近被访问过的状态，也就是说，最近且最频繁被访问到的状态会被赋予最大的“资格”。更新方式为：

% （1）首先，计算当前状态的TD偏差:$\delta_{t}=R_{t+1}+\gamma V(S_{t+1})-V(S_{t})$

% （2）其次，更新资格迹：$Z_{t}(s) = 
%     \begin{cases}
%         \gamma \lambda Z_{t-1} & if s \neq s_{t},\\
%         \gamma \lambda Z_{t-1}＋1 & if s = s_{t}.
%     \end{cases}$

% （3）最后，对状态空间中的每个状态$s$，更新值函数：$V(s) \gets V(s)+\alpha \delta_{t} Z_{t}(s)$

% 综上，后向视角的时间差分方法不仅不需要环境模型，而且利用了增量在线的机制，实现方式简单有效，同时也保证了策略的实时性，被认为是强化学习中最核心的算法。

\section{值函数的逼近方法}
至此，可以归纳出基于值函数的强化学习算法的基本步骤：首先进行值函数的评估，然后再利用值函数改善当前的策略。其中，值函数的评估是关键。

在之前所介绍的强化学习方法中，值函数其实是一个表格，其索引是状态或者状态行为对，值迭代更新实际上就是这张表格的迭代更新，因此，这种强化学习称为表格型强化学习，它有一个前提条件：状态空间和行为空间都是离散的，且空间规模不能太大。然而，现实中的很多问题都具有很大的状态（行为）空间，甚至是连续的状态（行为）空间，比如围棋有 $10^{170}$  个状态空间，控制直升机飞行需要的是一个连续的状态空间。在这种情况下，可以使用函数逼近的方法表示值函数。

从数学的角度来看，函数逼近方法可以分为参数化函数逼近方法和非参数化函数逼近方法，其中参数化函数逼近根据函数逼近器种类的不同又可以分为参数化线性函数逼近和参数化非线性函数逼近。

\subsection{参数化函数逼近}

\paragraph{参数化线性函数逼近}
参数化函数逼近方法是指从参数空间到近似值函数空间的映射，因为近似的状态(状态行为)值函数可以由一组参数$\bm{\theta}\in \mathbb{R}^{n} $ 来表示，在本文中，将逼近的状态值函数和状态行为值函数分别记为：$\hat{v}(s;\bm{\theta})$和$\hat{q}(s,a;\bm{\theta})$，如\eqref{seq_2_3_2}所示。
% 函数的形式和参数的个数事先由先验知识预设，而且参数是通过与目标函数有关样本数据来调整的。
\begin{equation}
\label{seq_2_3_2}
\begin{aligned}
&\hat{v}(s;\bm{\theta})\approx v_{\pi}(s)\\
&\hat{q}(s,a;\bm{\theta})\approx q_{\pi}(s,a)
\end{aligned}
\end{equation}

从上式$\eqref{seq_2_3_2}$中，可以发现，在近似的状态值函数$\hat{v}(s;\bm{\theta})$中，针对每个状态，不再存储其状态值，而是只存储一组参数。把从已知的状态中到的值函数，推广至那些未碰及的状态中，这对于拥有大规模的状态空间来说，相当于对其状态空间进行了压缩。由于 $\bm{\theta}\in \mathbb{R}^{n}$，因此所需的存储开销为$O(n)$，当状态空间规模较大且均为离散时，$n$远远小于为每个状态存储值的开销$|S|$。

当逼近值函数的结构确定时，那么值函数的逼近就等价于参数的逼近。值函数的更新也就等价于参数的更新，也就是说，需要利用试验数据来更新参数值。但是，强化学习中没有标签数据，如何才能才能从数据中学到值函数呢？

从表格型值函数的更新过程中，可以看出无论是蒙特卡罗方法还是时间差分方法，都是朝着一个目标值更新的，这个目标值在蒙特卡罗方法中是$G_{t}$，在时间差分方法中是$R_{t+1}+\gamma Q(S_{t+1},A_{t+1})$
% ，在$TD(\lambda)$中是$G^{\lambda}_{t}$
。所以，如果将表格型值函数的更新过程推广至值函数逼近过程，那么，逼近值函数$\hat{v}(s;\bm{\theta})$的过程实际上可以看作是一个监督学习的过程，其数据和标签对为$<S_{t}, v_{\pi}(S_{t})>$，其中$v_{\pi}(S_{t})$等价于蒙特卡罗方法中的$G_{t}$，时间差分方法中的$R_{t+1}+\gamma Q(S_{t+1},A_{t+1})$
% ,以及$TD(\lambda)$中是$G^{\lambda}_{t}$
。
% 因此，只要将公式$\eqref{seq:gradient}$总的$v_{\pi}(S_{t})$做相应替换就可以进行求解了。
那么，以状态值函数的逼近为例，训练的目标函数可表示为\eqref{seq:fun_approxi_obj}所示：
\begin{equation}
\label{seq:fun_approxi_obj}
\begin{aligned}
\argmin_{\bm{\theta}}(v_{\pi}(s)-\hat{v}(s;\bm{\theta}))^2
\end{aligned}
\end{equation}

即通过寻找参数向量$\bm{\theta}$，最小化近似值函数$\hat{v}(s;\bm{\theta})$与真实值函数$v_{\pi}(s)$的均方差。根据更新方式的不同，值函数更新方法可以分为增量式学习方法和批更新方法。其中，随机梯度下降法是最常用的增量式学习方法。

由公式$\eqref{seq:fun_approxi_obj}$，利用随机梯度下降法，可以得到参数的更新公式为\eqref{deq:suijitidu}：
\begin{equation}\label{deq:suijitidu}
\begin{aligned}
\bm{\theta}_{t+1} \gets \bm{\theta}_{t} + \alpha[v_{\pi}(s)-\hat{v}(S_{t};\bm{\theta})]\triangledown_{\theta} \hat{v}(S_{t};\bm{\theta})
\end{aligned}
\end{equation}

% 进而，得到值函数参数的更新如下：
% \begin{equation}
% \label{seq:gradient}
% \begin{aligned}
% \triangle \bm{\theta} = \alpha [v_{\pi}(s)-\hat{v}(s;\bm{\theta})] \triangledown_{\theta} \hat{v}(s;\bm
% {\theta})
% \end{aligned}
% \end{equation}

% 然而，公式$\eqref{seq:gradient}$并不能直接用于强化学习中，因为公式里有一个真实价值函数$v_{\pi}(s)$，或者是一个具体的数值，而强化学习没有监督数据。

在增量式的学习方法中，每次更新模型需要与环境交互，数据使用一次即抛弃，导致样本使用效率不高。

而所谓批更新方法是从给定一段时期的经验中抽取数据集：$D=\{<S_{1},v^{\pi}_{1}>,<S_{2},v^{\pi}_{2}>,\cdots,<S_{T},v^{\pi}_{T}>\}$，找到最好的拟合函数$\hat{v}(s;\bm{\theta})$，使的$LS(\bm{\theta})=\sum_{t=1}^{T}(v^{\pi}_{t}-\hat{v}^{\pi}_{t}(S_{t},\bm{\theta}))^2$ 最小，这种方法相当于经历重现（Experience Replay），把一段时期内的经历重新过一遍，更新参数，因此样本的利用效率高。

% 参数化函数逼近根据所使用逼近函数的不同，可分为参数化线性函数逼近和参数化非线性函数逼近两种。
以状态值函数为例，线性函数逼近可以表示为：
\begin{equation}
\begin{aligned}
\hat{v}(s;\bm{\theta})=\sum^{n}_{i=1}\phi_{i}(s) \bm{\theta}_{i}=\bm{\theta }^{T} \bm{\phi}(s)
\end{aligned}
\end{equation}

其中，$\bm{\theta}=(\bm{\theta}_{1},\cdots,\bm{\theta}_{n}) \in \mathbb{R}$，$\bm{\phi}(s)=(\phi_{1}(s),\cdots,\phi_{n}(s))^{T}$为状态$s$的特征向量，$\phi_{i}(s)$称为特征函数或者称为基函数，常见的基函数有：多项式基函数、傅立叶基函数以及径向基函数等。线性函数逼近不但形式简单、而且可以收敛到全局最优，缺点是表征能力较弱，而且基函数的形式需要事先选定，这也限制了函数的逼近能力。
% 但是正是由于其具备良好的收敛性和理论简单性，在强化学习的参数化方法中得到了充分应用。


\paragraph{参数化非线性函数逼近}
在非线性函数逼近方法中，逼近函数是关于参数$\bm{\theta}$的非线性函数。在众多的非线性方法中，因为神经网络关于状态是可导的而且可以迭代求解，所以是强化学习中最适合的非线性函数逼近器。

神经网络可以也看作一种参数化基函数的方法，由于基函数的参数是从数据中学习得到的，因此其表示能力大大提升，神经网络的每层都可看成是基于上一层的新的基函数，也就是特征。从这个意义上说，后面一层是前一层的抽象，从而可以对高维输入达到降维的效果。所以，和线性逼近器相比，神经网络的优点是有很强的表征能力和逼近能力，可以对目标函数以任意精度逼近，但是也面临着许多问题：深度学习的样本之间是独立的，且目标分布是固定的，而强化学习前后的状态是相关的，并且目标分布在一直变化，往往会出现收敛不稳定等问题。因此，需要一个适用于非静态、非独立均匀分布的数据的训练方法来得到近似函数。

针对以上问题，基于Q-learning算法DeepMind团队提出了解决方法。1）使用卷积神经网络进行值函数的逼近。2）通过经验回放（Experience Replay）机制来解决强化学习中相关性以及非静态分布的问题。3）通过设置独立目标网络(Target Net)来单独处理时间差分算法中的TD偏差，进一步降低数据之间的关联性，从而削弱收敛不稳定的问题。这就是DQN模型的主要内容，因为本文的第四章的模型主要基于DQN模型的，所以，此处将这三个改进点进行介绍。

% 在强化学习的应用场景中，状态数据是可能是持续流入的，而且下一个状态通常与前一个状态是高度相关的。
% 但是其收敛性难以保证，容易陷入局部最优，使的学术界对其的研究停滞了很长时间，直到2013年，DeepMind团队提出了一些改进的深度强化学习的训练方法，很大程度上解决了神经网络在强化学习问题中的收敛性问题，并且取得了令人振奋的实验表现，引发了针对此问题的新的研究热潮。具体细节可以参见本文第三章关于DQN的介绍。深度神经网络就是其中一种参数化非线性函数逼近器，它不但可以高效的解决高维且连续的问题，而且还可以从数据中自动提取复杂的特征，于是出现了很多基于深度学习的深度强化学习（Deep Reinforcement Learning, DRL）技术。深度神经网络和强化学习技术的完美结合，可以利用神经网络强大的非线性逼近能力，让强化学习更好的进行Q值函数的学习，这种结合虽然可以在一定程度上达到优势互补的作用，但是也面临着许多问题。1）深度学习需要大量带标签的样本进行训练，而强化学习只有奖赏的返回值，并且存在奖赏延迟
% 在众多DRL模型中，以DQN模型最具代表性，因为它解决了DRL中一些一直以来悬而未解的关键问题，才使得深度强化学习以惊艳的表现重新回到大众的视野中。

 \subparagraph{卷积神经网络逼近值函数与构造标签}
图\ref{fig:CNN_DQN}为DQN模型逼近Q值函数时所使用的深度卷积神经网络的结构，它包含两个卷积层加两个全连接层。利用神经网络逼近值函数，不仅具有很强的表征能力，而且还可以自动自动提取复杂的特征。此处的Q值函数对应着一组参数，也就是神经网络中每层网络的权重，可以用向量$\bm{\theta}$表示，对应的值函数可以表示为$Q(s,a;\bm{\theta})$，所以对值函数的更新也就是对参数向量$\bm{\theta}$的更新。
\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{CNN_DQN}
\caption{DQN模型值函数逼近网络结构}
\label{fig:CNN_DQN}
\end{figure}

神经网络的训练是一个最优化过程，目的是让损失函数最小化，而损失函数是标签和网络输出的偏差。为此，需要巨量的带有标签的数据，然后通过反向传播使用梯度下降的方法来更新神经网络的参数。然而，Q-learning算法只有奖赏的返回值，参考上文的描述，可以使用Q目标值作为标签进行训练。

在算法$\ref{algo:algorithm_2}$中，Q的目标值是$r+\gamma \max_{a}Q(s^{'},a)$，而我们学习Q值函数的目的就是趋近该目标值。因此，DQN网络的损失函数可表示为：
\begin{equation}
\label{seq_3_2_1}
\begin{aligned}
L(\theta)=\mathbb{E}[(\underbrace{r+\gamma\max_{a^{'}} Q(s^{'},a^{'};\bm{\theta})}_{Target}-Q(s,a;\bm{\theta}))^{2}]
\end{aligned}
\end{equation}

在公式$\eqref{seq_3_2_1}$中，$s^{'}$，$a^{'}$分别表示下一个状态和下一个时刻将要采取的行为。
% 这里使用Q-learning要更新的Q值值函数作为目标值。有了目标值，又有当前值，那么偏差就能通过均方差来进行计算了。
接着，就可以按照参数化函数逼近的训练步骤，先求出损失函数的梯度，然后利用随机梯度下降法(Stochastic Gradient Descent, SGD)来更新参数，从而得到最优的Q值函数了。

其实，早在1995年Bertsekas等人就将神经网络应用在在强化学习的值函数逼近中，取得了相比线性逼近较好的结果，但是往往会出现不稳定不收敛的情况\citep{bertsekas1995neuro}。此后，众多学者在这个方向上一直没有突破，直到DeepMind团队提出以下两点改进方法。

 \subparagraph{设置经验回放机制}
DeepMind团队的创始人Hassabis是神经科学的博士，他主要研究人类大脑中的海马体，海马体是大脑中主要负责记忆和学习的部分。他在研究时发现，人类在睡觉的时候，海马体会把一天的记忆重放给大脑皮层，利用这个启发机制，DeepMind团队的研究人员设计了一种神经网络的训练方法：经验回放。

如图$\ref{fig:experience_reply}$所示，经验回放是将探索环境得到的转移样本（$S_{t}, A_{t}, R_{t+1}, S_{t+1}$）存储在一个数据库（回放记忆单元）中，再利用随机均匀采样的方法从数据库中抽取转移样本，来训练神经网络。因为在神经网络的训练过程中，假设训练数据是独立同分布的，但是通过强化学习采集的数据之间存在关联性，如果利用这些数据进行顺序训练，神经网络难免会不稳定。利用经验回放机制可以打破这种数据间的关联性，从而解决了数据相关性以及数据的非静态分布的问题。
\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{experience_reply}
\caption{经验回放机制}
\label{fig:experience_reply}
\end{figure}

 \subparagraph{设置独立的目标网络}
使用梯度下降法对参数$\bm{\theta}$进行更新时，可表示为式$\eqref{dituxiajiangq}$：
\begin{equation}\label{dituxiajiangq}
\begin{aligned}
\bm{\theta}_{t+1}=\bm{\theta}_{t}+\alpha[\underbrace{r+\gamma \max_{a^{'}}Q(s^{'},a^{'};\bm{\theta})}_{Target}-Q(s,a;\bm{\theta})]\triangledown Q(s,a;\bm{\theta})
\end{aligned}
\end{equation}

其中，$r+\gamma \max_{a^{'}}Q(s^{'},a^{'};\bm{\theta})$为Q目标值，在计算$\max_{a^{'}}Q(s^{'},a^{'};\bm{\theta})$时用到的参数向量为$\bm{\theta}$。在DQN算法出现之前，利用神经网络逼近值函数时，计算Q目标所用的网络参数向量也是$\bm{\theta}$，与梯度计算$\triangledown Q(s,a;\bm{\theta}$时所使用的参数向量相同，这样就容易导致数据间存在关联性，从而使训练不稳定。

为了解决此问题，DeepMind团队提出使用另一个目标网络（TargetNet）产生Q目标值。具体地，$\bm{\theta}$代表当前网络（MainNet）的输出，用来评估当前状态行为对的值函数；$\bm{\theta}^{-}$代表目标网络的输出，以此求出Q目标值。并且，用于状态行为值函数逼近的网络每一步都要更新，而用于计算目标的网络则是每个固定的步骤更新一次。

因此，值函数的更新变为：
\begin{equation}
\begin{aligned}
\bm{\theta}_{t+1}=\bm{\theta}_{t}+\alpha[\underbrace{r+\gamma \max_{s^{'}}Q(s^{'},a^{'};\bm{\theta}^{-})}_{TargetNet}-\underbrace{Q(s,a;\bm{\theta})]\triangledown Q(s,a;\bm{\theta})}_{MainNet}
\end{aligned}
\end{equation}

\subsection{非参数化函数逼近}
在参数化函数逼近过程中，基函数的形式和参数的个数都需要提前设定，并且值函数的逼近效果很大程度上受到人为的经验的影响。而在非参数化函数逼近中，参数的个数以及基函数的形式并不是固定的，而是由样本决定，具有很高的灵活性。非参数化函数逼近方法包括基于核函数的方法和基于高斯过程的方法。因为本文第四章是使用的基于核函数的非参数化函数逼近的方法，所以此处仅对基于核的非参数函数逼近进行介绍。

基于核函数逼近模型可以表示为
\begin{equation}\label{seq2_3_0}
\begin{aligned}
&\hat{q}(s,a;\bm{\theta})=\sum^{n}_{i=1}\bm{k}((s,a),(s_{i},a_{i}))\bm{\theta}_{i}\\
&\hat{v}(s;\bm{\theta})=\sum^{n}_{i=1}\bm{k}((s),(s_{i}))\bm{\theta}_{i}
\end{aligned}
\end{equation}

式$\eqref{seq2_3_0}$中，$\bm{\theta}_{1},\cdots,\bm{\theta}_{n}$为参数向量，$\{(s_{i},a_{i})|i=1,\cdots,n\}$为样本集合,$\bm{k}:S\times A \times S \times A \to \mathbb{R} $为核函数。其中，常用的核函数有线性核函数、多项式核函数、径向基核函数以及Sigmoid核函数等。

利用核函数法逼近值函数的关键是能够将问题构造成一个带有核函数的优化问题。非参数化函数学习过程中完全依赖于样本，虽然带来一定的逼近灵活性，但是收敛性难以得到保证，而且随着样本规模的扩大，其计算复杂度呈指数级上升。

\section{本章小结}
 本章主要介绍了强化学习的相关理论基础知识。首先，对强化学习的整体交互过程进行简单介绍，让读者可以对强化学习有一个直观的认识和体会；接着，介绍了强化学习的框架马尔科夫决策过程以及强化学习的三个要素：策略、回报和值函数；然后，介绍了强化学习基于模型和模型无关的三种求解方法；最后，针对大规模问题，介绍了求解值函数的两种逼近方法：参数化函数逼近以及非参数化函数逼近。为后面章节做了铺垫。