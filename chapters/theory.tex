\chapter{相关理论知识}
强化学习主要解决序贯决策问题：即需要连续不断地做出决策，才能实现最终的目标，而它的目标是求解一个可以使得回报期望最大化的最优策略。一般的序贯决策问题可以利用马尔科夫决策过程（Markov Decision Process, MDP）的框架来描述，在利用马尔科夫决策过程将问题形式化后，可以使用基于模型的动态规划方法和基于无模型的强化学习方法来解决MDP问题，其中，值函数的估计的方法又包括参数化函数逼近和非参数化函数逼近两种。本章会依次对它们进行简要的介绍。

\section{马尔科夫决策过程和强化学习}
% 介绍马尔科夫决策过程，策略、累计回报、值函数、贝尔曼方程、最优值函数
\paragraph{马尔科夫决策过程}
马尔科夫决策过程是强化学习的理论基础，通常使用一个四元组$<S,A,p,r>$表示，其中：$S$表示有限的状态空间，定义为一个有穷集合$\{ S_1,S_2,\cdots ,S_N \}$，$N$为状态空间的大小。$A$表示有限的行为空间，定义为一个有穷集合$\{ A_1,A_2,\cdots ,A_M \}$，$M$为行为空间的大小。$p$表示状态转移概率，$r$表示奖赏函数。

在任意的离散时间点$t$时，环境状态为$S_{t}$，此刻若系统采取行为$A_t$，则会得到有限的即时奖赏$R_{t}$，并且状态也会转移到下一个状态$S_{t+1}$。奖赏$R_{t}$是根据奖赏函数$r$得到的，下一时刻的状态$S_{t+1}$是根据状态转移概率$p$得到的，又因为马尔科夫决策过程的状态具有马尔科夫性，因此状态转移函数和奖赏函数都仅和当前状态和行为有关，与历史的状态序列和行为序列无关。即：$p(s^{'},r|s,a) = Pr\{S_{t}=s^{'}, R_t=r|S_{t-1}=s, A_{t-1}=a\}$且$\sum_{s^{'}\in S}p(s^{'}|s,a)=1$，$\forall s\in S,\forall a\in A$。根据状态转移的情况，可以分为确定性状态转移和随机性状态转移。确定性状态转移中，会转移到确定性的下一状态，而在随机性状态转移中，迁移到的下一状态是不确定的，是一个随机变量。

\paragraph{策略和回报}
% \subparagraph{策略}
强化学习的目标是从给定马尔科夫决策过程中，寻找最优的策略。所谓策略是指在某一状态下，所采取的动作或所采取动作的概率，通常用符号$\pi$表示：$\pi(s,a)=p[A_{t}=a|S_{t}=s]$。策略也分为确定性策略和随机性策略，其中确定性策略输出的是一个动作序列，而随机性策略输出的是该状态下所采取动作的概率。

% \subparagraph{回报}
当给定一个策略$\pi$后，如何进行策略的评估呢？即用到了累积奖赏（cumulative reward）即回报（return）的概念。假设在时刻$t$后接收到的奖赏序列为$\{R_{t+1}, R_{t+2},\cdots\}$，通常采用折扣累积回报的方式计算回报，那么：
\begin{equation}
G_{t}=\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}
\end{equation}
式中，$\gamma<1$，为一个常量，称为折扣因子。在Agent与环境的交互过程中，如果任务可以自然地被分为带有终止时间片的片段，则该任务称为情节式（episodes）任务。如果任务无法分解成若干片段，整个任务需要不断的进行下去，则改任务称为连续式（continous）任务。

\paragraph{值函数}
我们注意到上一节中的累积回报$G_{t}$是个随机变量，不是一个确定的值，无法作为目标函数进行优化。但是其期望是个确定值，因此可以作为目标函数，并定义为状态值函数，用来评估状态$s$的价值。即：当Agent采取策略$\pi$时，回报服从一个分布，且回报在状态$s$的期望值定义为策略$\pi$的状态值函数：
\begin{equation}
v_{\pi}(s)=\mathbb{E}_{\pi}[G_{t}|S_t=s]=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}|S_t=s]
\end{equation}
相应地，策略$\pi$的状态行为值函数可以用来评估在状态$s$下采取行为$a$的价值，公式化为：
\begin{equation}
q_{\pi}(s,a)=\mathbb{E}_{\pi}[G_{t}|S_t=s,A_t=a]
=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}|S_t=s,A_t=a]
\end{equation}
因为上述状态值函数的表达形式在实际应用中不方便表达，因此，我们可以进行进一步的推导，得到状态值函数的贝尔曼方程：
\begin{equation}
\label{seq1}
\begin{aligned}
v_{\pi}(s)&=\mathbb{E}_{\pi}[G_{t}|S_t=s]\\
&=\mathbb{E}_{\pi}[R_{t+1}+\gamma G_{t+1}|S_t=s]\\
&=\sum_{a}\pi(a|s)\sum_{s^{'}}\sum_{r^{'}}p(s^{'},r|s,a)[r + \gamma\mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s^{'}]]\\
&=\sum_{a}\pi(a|s)\sum_{s^{'},r}p(s^{'},r|s,a)[r+\gamma v_{\pi}(s^{'})], \forall s \in S
\end{aligned}
\end{equation}

同样地，也可以得到状态行为值函数的贝尔曼方程：
\begin{equation}
\label{seq2}
\begin{aligned}
q_{\pi}(s,a)=\sum_{s^{'},r}p(s^{'},r|s,a)[r+\gamma \sum_{a'}\pi(a'|s') q_{\pi}(s^{'},a^{'})], \forall s \in S, \forall a \in A
\end{aligned}
\end{equation}
计算值函数的目的是为了从数据中学习到最优策略，而每个策略对应着一个状态值函数，那么最优策略自然对应着最优状态值函数。所谓的最优状态值函数$v_{*}(s)$是指所有策略中值最大的值函数，即$v_{*}(s)＝=\max_{\pi}v_{\pi}(s)$，同样地，最优状态行为值函数$q_{*}(s,a)$为在所有策略中值最大的状态行为值函数，即$q_{*}(s,a)=\max_{\pi}q_{\pi}(s,a)$

因此，我们又可以由式\eqref{seq1}和式\eqref{seq2}进行推导（因为篇幅限制，推导过程略），分别得到最优状态值函数和最优状态行为值函数的贝尔曼最优方程：
\begin{equation}
\begin{aligned}
v_{*}(s)=\max_{a}\sum_{s^{'},r}p(s^{'},r|s,a)[r+\gamma v_{\pi}(s^{'})]
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
q_{*}(s,a)=\sum_{s^{'},r}p(s^{'},r|s,a)[r+\gamma \max_{a'} {\sum_{a'}\pi(a_{'}|s_{'})} q_{\pi}(s^{'},a^{'})]
\end{aligned}
\end{equation}
最后，若求出了最优状态行为值函数，最优策略可以通过直接最大化 $q_{*}(s,a)$来决定：
\begin{equation}
\begin{aligned}
\pi_{*}(a|s) = 
    \begin{cases}
        1 & if \ a=\argmax_{a\in A}q^{*}(s,a),\\
        0 & otherwise.
    \end{cases}
\end{aligned}
\end{equation}

\section{强化学习经典算法}
根据是否依赖于环境，强化学习可以分为基于模型的动态规划法和模型无关的强化学习算法。在基于模型的动态规划法中，需要提供精确的环境模型而不存在模型学习的过程，然后根据环境的动态性利用贝尔曼公式迭代的求解状态值函数或者状态行为值函数，其本质是动态规划（Dynamic Programming, DP）的思想，通过不断迭代直至稳定，得到精确解。模型无关的强化学习算法无需提供环境模型，根据agent与环境的不断交互收集样本，然后直接利用样本求解状态值函数和状态行为值函数。主要包括蒙特卡罗（Monte Carlo, MC）法和时间差分（Temporal Difference, TD）等算法。

\subsection{动态规划法}
动态规划法是指在给定模型的情况下，计算MDP最优策略的方法。主要包括策略迭代和值迭代两种方法。

策略迭代算法包括策略评估和策略改善两个步骤，且两个步骤交替进行。在策略评估中，算法的每次迭代是建立在上一轮策略改善的基础上，对状态空间中的每个状态进行扫描，并利用贝尔曼公式进行更新，经过不断的迭代，值函数最终收敛至不动点。在策略改善中，算法利用上一轮策略评估得到的值函数，以贪心的方式生成一个新的策略。两
个过程不断交替，直至策略收敛至最终策略。值得一提的是，在贝尔曼公式$\eqref{seq1}$中状态的值函数$v_{\pi}(s)$可以利用其后继状态的值函数$v_{\pi}(s^{'})$来表示，这是利用了自举（bootstrapping）的思想。

以行为状态值函数的策略迭代过程为例，：
\begin{displaymath}
\begin{aligned}
\pi_{0}\xrightarrow{E}q_{\pi_0}\xrightarrow{I}\pi_{1}\xrightarrow{E}q_{\pi_1}\xrightarrow{I}\pi_{2}\xrightarrow{E}q_{\pi_2}\xrightarrow{I}\pi_{3}\xrightarrow{E} \cdots \xrightarrow{I}\pi_{*}\xrightarrow{E}q_{\pi_{*}}
\end{aligned}
\end{displaymath}
其中$E$代表策略评估过程，$I$代表策略改善过程。在收敛到最优策略时，每一轮的策略都好于前一轮的策略。初始时，策略和值函数都是随机的。

在策略评估时，针对每一个行为状态值，使用贝尔曼公式$\eqref{seq2}$进行更新值函数，直到$q_{k+1}$稳定而结束本轮迭代。
\begin{equation}
\begin{aligned}
q_{k+1}(s,a)=\sum_{s^{'},r}p(s^{'},r|s,a)[r+\gamma \sum_{a'}\pi(a^{'}|s^{'}) q_{k}(s^{'},a^{'})]
\end{aligned}
\end{equation}
在策略改善时，对所有的状态，使用贪婪方法求出新的策略。
\begin{equation}
\begin{aligned}
\pi_{k+1}(s)=\argmax_{a}q_{k+1}(s,a)
\end{aligned}
\end{equation}
当策略$\pi$稳定时，迭代过程结束，即得到最优值函数$q_{*}(s,a)$和最优策略$\pi_{*}$。

值迭代算法是对策略迭代的简化，也包括策略评估和策略改善两个环节。与策略迭代不同的是，它不需要等到在策略评估值函数完全收敛时才进行下一次的迭代，而是对全部的状态行为空间每进行一次扫描（更新）就进行策略改善，加快的收敛速度。对每一个状态行为对，值迭代的更新公式如下：
\begin{equation}
\begin{aligned}
q_{k+1}(s,a)=\sum_{s^{'},r}p(s^{'},r|s,a)[r+\gamma \max_{a^{'}} q_{k}(s^{'},a^{'})]
\end{aligned}
\end{equation}
直到$q_{k+1}$稳定，则算法结束。此时因为值函数已经收敛，直接对值函数使用贪心方法就可以得到最优策略。


\subsection{蒙特卡洛方法}
在解决强化学习问题时，由于动态规划法需要提供完整的环境模型且计算代价太大等问题，限制了其实际应用。当没有模型时，我们可以采用蒙特卡罗方法计算值函数，即用经验平均的思想代替随机变量的思想。其中，经验是指按照该策略做了很多次试验，产生很多情节episode，每一个情节就是一次实验，平均就是平均值，按照求解方法不同又可以分为第一次访问蒙特卡罗方法（英文，文献）和每次访问蒙特卡罗方法（英文，文献）。在与环境的交互过程中，当一个情节结束后，测发生值函数的更新和策略的改善。其中值函数的更新采取可递增均值的方法，公式为：
\begin{equation}
\begin{aligned}
v_{k+1}(s)=v_{k}(s)+ \alpha(G_{t}-v_{k}(s))
\end{aligned}
\end{equation}
其中，$\alpha$为学习率，$G_{t}$为从状态$s$出发至情节结束所获的的累积折扣奖赏。

但是，在动态规划中，为了保证值函数的收敛，算法会扫描状态中间的每个状态。而无模型的方法充分评估值函数的前提是每个状态都可以被访问到，因此蒙特卡罗方法采用了探索性初始化的方法，即在迭代每一个情节时，初始状态都是随机分配的，这样可以保证迭代过程中每个状态行为对都能被选中。

\subsection{时间差分方法}
相比于动态规划法，蒙特卡罗法使用了经验平均，虽然摆脱了对模型的依赖，但是需要等到每个情节结束后才可以进行值函数评估和策略更新，所以学习速度慢，学习效率不高。而动态规划因为使用了bootstrapping，所以可以在情节未结束时就可以根据未来值函数估计当前的值函数。时间差分法综合了两者的优点，融合了蒙特卡罗的采样方法和动态规划法的bootstrapping思想。

在TD(0)算法中是一步更新，即值函数直接根据下一个时间步进行学习，估计下一个状态的期望回报。值函数的更新公式为：
\begin{equation}
\begin{aligned}
v(S_{t})=v(S_{t})+\alpha (R_{t+1}+\gamma v(S_{t+1})-v(S_{t}))
\end{aligned}
\end{equation}
式中，$\alpha$为步长参数，控制学习率。$R_{t+1}+\gamma v(S_{t+1})$称为TD目标，$\delta_{t}=R_{t+1}+\gamma v(S_{t+1})-v(S_{t})$为TD偏差。

根据探索策略（行为策略）和评估策略是否为一个策略，可以将强化学习方法分为同策略（on-policy）和异策略（off-policy）两种方法。同样时间差分方法也包括了同策略的Sarsa方法和异策略的Q-learning方法。Sarsa方法中，行动策略和评估策略都是$\epsilon-greedy$的方法，对应的算法伪代码如$\ref{algo:algorithm_1}$所示。

\begin{algorithm}[htbp]
\small
\SetAlgoLined
\SetKwRepeat{Repeat}{repeat}{until} 
% \KwData{this text}
% \KwResult{how to write algorithm with \LaTeX2e }

初始化$Q(s,a)$，$\forall s \in S$，$\forall a \in A$， 给定参数$\alpha$，$\gamma$\;
\Repeat{所有的$Q(s,a)$收敛}{
初始化其实状态$s$\;
根据$\epsilon-greedy$策略在状态$s$下选择行为$a$\;
\Repeat(（对每个情节的每一步）){$s$是终止状态}{
	根据$\epsilon-greedy$策略在状态$s$下选择行为$a$，得到回报$r$和下一状态$s^{'}$，在状态$s^{'}$根据$\epsilon-greedy$策略得到动作$a^{'}$\;
	$Q(s,a)=Q(s,a)+\alpha[r+\gamma Q(s^{'},a^{'}-Q(s,a))]$\;
	$s=s^{'}$，$a=a^{''}$\;
	}
}
输出最终策略：$\pi(s)=\argmax_{a}Q(s,a)$\;
\caption{Sarsa算法伪代码}
\label{algo:algorithm_1}
\end{algorithm}

与Sarsa方法不同，在Q-learning中，行为策略采用$\epsilon-greedy$策略，而目标策略采用贪婪策略，故又称为异策略Q-learning算法，其伪代码如$\ref{algo:algorithm_2}$所示。
\begin{algorithm}[htbp]
\small
\SetAlgoLined
\SetKwRepeat{Repeat}{repeat}{until} 
% \SetAlgoRefName{algorithm_2}
初始化$Q(s,a)$，$\forall s \in S$，$\forall a \in A$， 给定参数$\alpha$，$\gamma$\;
\Repeat{所有的$Q(s,a)$收敛}{
初始化其实状态$s$\;
根据$\epsilon-greedy$策略在状态$s$下选择行为$a$\;
\Repeat(（对每个情节的每一步）){$s$是终止状态}{
	根据$\epsilon-greedy$策略在状态$s$下选择行为$a$，得到回报$r$和下一状态$s^{'}$\;
	$Q(s,a)=Q(s,a)+\alpha[r+\gamma \max_{a} Q(s^{'},a^{'}-Q(s,a))]$\;
	$s=s^{'}$，$a=a^{''}$\;
	}
}
输出最终策略：$\pi(s)=\argmax_{a}Q(s,a)$\;
\caption{Qlearning算法伪代码}
\label{algo:algorithm_2}
\end{algorithm}

在TD(0)中，更新当前值函数时，只用到了下一步状态值函数。而TD($\lambda$)考虑从未来的多步进行学习，并且采用加权的方法融合这多步的估计值，$\lambda \in [0,1]$决定了向未来观察的时间步长度。即
\begin{equation}
\begin{aligned}
V(S_{t})=V(S_{t})+\alpha (G^{(\lambda)}_{t}-V(S_{t}))
\end{aligned}
\end{equation}
其中，
\begin{displaymath}
\begin{aligned}
G^{(\lambda)}_{t}=(1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G^{n}_{t}\\
\end{aligned}
\end{displaymath}
\begin{displaymath}
\begin{aligned}
G^{n}_{t}=R_{t+1}+\gamma R_{t+1}+ \cdots +\gamma^{n-1} R_{t+n}+\gamma^{n} V(S_{t+n})
\end{aligned}
\end{displaymath}

这就是TD($\lambda$)的前向视角表达形式，这种方式同样需要等到整个实验结束才能计算。后向视角利用增量式的更新方式，不需要等到实验结束就可以更新当前状态的值函数，并且引入了资格迹$E_{t}(s)$的概念，资格迹记录了最近被访问过的状态，也就是说，最近且最频繁被访问到的状态会被赋予最大的“资格”。更新方式为：

首先，计算当前状态的TD偏差:$\delta_{t}=R_{t+1}+\gamma V(S_{t+1})-V(S_{t})$

其次，更新资格迹：$E_{t}(s) = 
    \begin{cases}
        \gamma \lambda E_{t-1} & if s \neq s_{t},\\
        \gamma \lambda E_{t-1}＋1 & if s = s_{t}.
    \end{cases}$

最后，对状态空间中的每个状态$s$，更新值函数：$V(s)=V(s)+\alpha \delta_{t} E_{t}(s)$

综上，时间差分方法不仅不需要环境模型，而且它利用增量在线的机制，实现方式简单有效，同时也保证了策略的实施性，称为强化学习中最核心的算法。

\section{值函数逼近方法}
\subsection{参数化函数逼近}
\subsection{非参数函数化逼近}

\section{本章小结}
 