
\chapter{基于Q-learning的直复营销策略}

\section{研究动机}
直复营销是一个序贯决策过程，即随着时间的推移，营销人员需要不断的做出营销决策。所以，在进行决策时，营销人员不仅需要考虑到每个决策行为的成本和其收益之间的关系，而且还要考虑到营销序列上不同决策点之间的相互影响。

考虑如下场景：在某次营销决策时，营销人员发现如果对某一位客户进行营销，其产生的估计收益会大于营销成本，那么此时就不会对这位客户进行营销。但是，应该意识到即使此时该客户没有产生利润，但是这次的营销可能会增加该客户在以后的营销活动的中产生的利润，甚至该利润会很大。所以，营销人员在进行决策时，有时需要牺牲即时收益以获得长期收益最大化。反之亦然，如果频繁的对某一位高质量用户发送营销信息，会降低该客户所能产生的期望收益，因为每一位客户在一段时间内消费能力是有限的。

而传统的经典分类算法和基于代价敏感的改进分类算法，都只考虑到最大化单个独立决策事件的收益，并不能保证在一段时间上的长期收益最大化。考虑到强化学习是以累积收益最大化为目标，并且基于序列中的延迟影响来进行决策的学习，通过这种方式可以很好的处理序列决策中相互影响，进而达到长期收益最大化的目标，所以文献\citep{,pednault2002sequential,archak2010budget,boutilier2016budget}等提出使用强化学习的思想来解决直复营销问题。但是，在上述相关工作中，以下问题仍然没有得到解决：各营销决策点之间存在可变时间间隔会给奖赏数据带来一定的噪声影响，而且，随着数据规模的不断提升，值函数的收敛速度和学习速度都会变慢。

本章针对以上两个问题进行分析，提出了相应的改进方法，以更好的解决直效营销问题。首先，利用马尔科夫决策过程，将直复营销问题建模为一个强化学习问题，并给出Q-learning算法解决该问题的框架；然后，为了解决营销决策点之间的可变时间间隔问题，提出了改进的Q值函数更新方法；最后，为了适应大批量的学习和更新任务，在Q-采样的基础上，提出一种新的采样方法，以提高学习效率。

\section{改进的Q-learning算法在直复营销中的建模}

\subsection{直复营销场景建模}
在第二章中提到，强化学习问题可以使用一个马尔可夫决策过程来描述：在任意给定的时间点上，假设环境处于某个状态，当Agent采取一个行为时，它会收到一个有限的奖赏，并且环境会相应的转移到下一个状态中。Agent选择的行为的依据是可以最大化累计奖赏，该奖赏通常是以累计折扣的形式表示。

对应到直复营销场景中，客户的状态可以使用客户在某一时刻所具有的特征信息来表示，比如文献\citep{tkachenko2015autonomous}中提到的Recency-Frequency-Monetary（最近交易时间、交易频率和交易金额）等，营销人员所采取的营销行为可以表示为Agent的行为，客户在营销反馈中所产生的价值作为奖赏信息。

在某一时刻，某一位客户历史的营销和消费信息，可以用来表示该客户此时的状态，如果营销人员对其采取了营销行为，那么该顾客的状态会发生改变，根据一定的转移概率转移到下一状态，并且在这一过程客户可能会产生一定的奖赏信息。以上这个过程始终贯穿于客户和营销人员的交互关系之中。需要注意的是，在客户状态发生转移时，所产生的奖赏信息是指该客户产生的消费金额减去相关成本后的净利润，如果客户没有发生消费购买行为，那么此时的奖赏值为负。也就是说，强化学习应用到该场景的任务就是最大化客户生命周期的净利润值。

本文考虑采用Q-learning（算法：$\ref{algo:algorithm_2}$）的强化学习方法作为框架，来解决以上问题。所以，可以使用$\{<S_{t},A_{t},R_{t}>\}_{t=1}^{\infty}$三元组来表示营销人员的每一次的营销事件。其中，$S_{t}$表示用户的状态，$A_{t}$表示营销人员所采取的行为，$R_{t}$表示此次营销所带来的奖赏。

\subsection{Q-learing的模型构建}
在传统的Q-learning算法中，值函数其实是一个表格，其索引是状态或者状态行为对，值迭代更新实际上就是这张表格的迭代更新。所以，Q-learning算法存在一个假设是，问题的状态空间和行为空间不能太大。然而，在像直复营销这类现实问题中，为了合理准确的表示客户的状态信息，需要非常多的特征进行表示，其状态空间自然非常大，所以，这时使用表格的形式进行表示并不现实。

如第二章所述，对于值函数的评估可以通过函数逼近的方法进行解决。虽然在非参数化函数逼近中，基函数的个数和形式由采样点决定，具有很好的表现力，但是非参数化函数逼近存在着随着数据增多计算量呈指数升高的问题，不适合直复营销这类含有大量训练数据的情况。另外，参数化非线性逼近方法在逼近过程中存在着收敛困难的问题，所以，本章采用考虑采用参数化线性逼近的方法进行值函数的逼近。同时，结合针对离散行为的分块逼近思想，提升模型的逼近精度。

\paragraph{多元线性分块逼近模型}
考虑具有连续状态空间$S=\{S_{t}|t\in \mathbb{R}\}$和离散行为空间$A=\{A_{t}\}_{t=1}^{N}$的强化学习问题，可以将离散行为根据行为值的不同划分为$N$块，然后利用逼近模型M对该问题的Q值函数进行建模。$M=\{M_{i},\cdots,M_{N}\}$，其中$N$为划分的块数，每一块对应一类离散行为的数据样本集合，每个样本集合对应一个子模型，各子模型之间相互独立。

子模型可以采用任意的结构模型，若$N$个子模型结构相同，则称M为同构分块逼近模型，否则称为异构分块逼近模型。基于以上分块逼近的思想，本节利用多元线性回归的方法构建一组同构的逼近模型，来共用逼近Q值函数。我们将不同行为对应的多元线性模型记为：$Q_{i}$，$i=1,\cdots,N$。

% \paragraph{参数化线性逼近器的构建}
% 下面详细介绍该模型的构建和逼近过程。
% 假设当前第$i$个行为对应的样本集合为$D_{i}=\{<\bm{X}_{ij}, Q_{ij}>\}_{j=1}^{N_{i}} \subseteq (\bm{S} \times A \times Y)^{N_{i}}$，其中$\bm{X}_{ij}=(\bm{S}_{ij}, A_{ij})$为样本$j$的输入向量，$Q_{ij}$为样本$j$的Q输出值，$N_{i}$为第$i$个行为对应的样本集合数量，$(\bm{S} \times A) = \mathbb{R}^{n+1}$表示输入域，$Y \subseteq \mathbb{R}$表示输出域。

\paragraph{模型的更新}
在传统的Q-learning算法中，还存在另一个假设，就是Agent可以与环境进行在线的互动。然而，像在直复营销这类实际应用中，构建这样的在线交互环境在很难的。为了解决这个问题，通常采用批强化学习（Batch Reinforcement Learning）的方法进行解决\citep{lange2012batch}，批强化学习是强化学习的一种形式，即Agent采取的行为、环境发生的状态转移都不以在线的方式进行，而是使用代表先前的经验的大量静态训练数据进行离线的学习。训练数据由状态向量、动作值以及奖赏值所组成的三元组来构成。这种批学习的方式反映了实际应用的真实交互情况。

在第二章中，我们知道强化学习值迭代的过程可表示为式$\eqref{seq_value_inter}$的形式：
\begin{equation}\label{seq_value_inter}
\begin{aligned}
&Q^{(0)}(s,a) = R(s,a) \\
&Q^{(p+1)}(s,a) = R(s,a) + \gamma \sum_{s^{'}} p(s^{'}|s,a) \max_{a^{'}} Q^{(p)}(s^{'},a^{'})
\end{aligned}
\end{equation}

其中，$R(s,a)$是指期望的即刻奖赏，$p(s^{'}|s,a)$表示在状态$s$下，采取行为$a$后，环境转移到下一状态$s^{'}$的概率。

所以，给定好训练数据，以及模型的逼近方法后，参考上式$\eqref{seq_value_inter}$值迭代的更新方法，就可以进行Q值的估计了。具体地，在第一次的迭代过程中，使用多元线性逼近模型预测基于状态s和行为a的期望即刻奖赏值$R(s,a)$，在第二次以及之后的迭代过程中，再次使用上述逼近模型，依据Q值函数的更新公式$\eqref{seq_value_q}$进行更新，以得到改进的Q值函数。
\begin{equation}\label{seq_value_q}
\begin{aligned}
Q(S_{t}, A_{t}) \gets Q(S_{t}, A_{t}) + \alpha (r_{t+1} + \gamma \max_{a} Q(S_{t}, a) - Q(S_{t}, A_{t}))
\end{aligned}
\end{equation}

结合以上批强化学习方法和多元线性函数逼近模型，图$\ref{algo:SVR+Q}$给出了基于Q-learning模型更新的伪代码。

在伪代码$\ref{algo:SVR+Q}$中，按照批强化学习的训练方法，将训练数据集$D$（总样本集合）分成$I$个情节（episode），每个情节由一系列事件（event）组成，每个事件包含状态、行为和奖赏信息信息。在情节数据中，保持了事件原有的出现顺序，以此来重现真实的交互的过程。

\begin{algorithm}[htbp]
\small
\SetAlgoLined
\SetKwRepeat{Repeat}{repeat}{until} 
\KwData{折扣因子$\gamma$，正则化参数$C$，RBF核函数的宽度$\sigma$，最大迭代次数$P$，原始样本$D=\{e_{i}|i=1,\cdots,I\}$，$e_{i}=\{<S_{i,j}, A_{i,j}, R_{i,j}>|j=1,\cdots,l_{i}\}$，（$D$表示样本集合，$e_{i}$表示第$j$个情节，$l_{i}$表示$e_{i}$的长度）}
\KwResult{输出最终的逼近模型：$Q^{(P)}$}

\For{all $e_{i} \in D$}{
	初始化第$i$个情节的数据：$D_{i}^{(0)}=\{<S_{i,j}, A_{i,j}, R_{i,j}>|j=1,\cdots,l_{i}\}$\;
}
整合所有情节的数据，生成总样本集合：$D^{(0)}=\cup_{i=1,\cdots,I} D_{i}^{(0)}$\;
将总样本集合$D^{(0)}$，按照行为标签ID$(A_{i,j}) \in \{1,2,\cdots, K \}$分发到各子模型的样本集合中，并利用多元线性逼近模型分别逼近值函数：$Q_{k}^{(0)}$， $k=1,\cdots,K$\;
多路逼近Q值函数：$Q^{(0)}=\cup_{k=1,\cdots,K}，Q_{k}^{(0)}$\;
\For{$p=1$ \KwTo $P$}{
	\For{all $e_{i} \in D$}{
		\For{$j$ \KwTo $l_{i}-1$}{
			利用公式$\eqref{seq_value_q}$更新状态行为值：\;
			$v_{i,j}^{(p)}=Q^{(p-1)}(S_{i,j},A_{i,j}) + \alpha^{(p)} (R_{i,j} + \gamma \max_{a} Q^{(p-1)}(S_{i,j+1},a)-Q^{(p-1)}(S_{i,j},A_{i,j}))$\;
		}
		更新第$i$个情节的样本：$D_{i}^{(p)}=\{<S_{i,j}, A_{i,j}, v_{i,j}^{(p)}>|j=1,\cdots,l_{i}-1\}$\;
	}
	整合所有情节的数据，生成总样本集合。$D^{(p)}=\cup_{i=1,\cdots,I}D_{i}^{(p)}$\;
	将总样本集合$D^{(p)}$，按照行为标签ID$(A_{i,j}) \in \{1,2,\cdots, K \}$ 分发到各子模型的样本集合中，并利用多元线性逼近模型分别逼近值函数：$Q_{k}^{(p)}$，$k=1,\cdots,K$\;
	多路逼近Q值函数：$Q^{(p)}=\cup_{k=1,\cdots,K} Q_{i}^{(p)}$\;
}
\caption{Q-learning算法}
\label{algo:SVR+Q}
\end{algorithm}

在第一轮迭代中，首先要初始化每个情节中的数据，整合成总样本集合$D^{(0)}$，然后按照总样本集合中行为标签值的不同划分$K$子样本集合，然后利用多元线性回归模型对即刻奖赏信号进行预测，以此作为初步的估计Q值函数。在第二轮及之后的迭代过程中，结合上一次的估计值函数，利用Q值函数的更新公式$\eqref{seq_value_q}$更新每个情节中每一个事件的状态行为值。当每一轮训练结束后，就将更新后的样本情节进行整合，形成总样本集合$D^{(p)}$，并按照行为标签值的不同划分$K$子样本集合，然后利用多元线性回归模型逼近每个行为的Q值函数，再多路逼近Q值函数$Q^{(p)}$。当所有轮数迭代完毕后，输出最终的Q值函数。另外，对算法中学习率$\alpha_{i}$的选取，通常设置为$\alpha_{i}=\frac{1}{K}$，目的是让学习的步伐随着迭代次数的增加而不断减小。

\subsection{可变时间间隔问题}
在标准马尔科夫决策过程中，假设每个决策点之间的时间间隔是固定的，所以，如果假设在时刻$t$后接收到的奖赏序列为$\{R_{t+1}, R_{t+2},\cdots\}$，那么采用折扣累积奖赏的方式计算回报可用公式$\eqref{seq:reward_3}$来表示：
\begin{equation}\label{seq:reward_3}
G_{t}=\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}
\end{equation}
式$\eqref{seq:reward_3}$中，$G_{t}$为回报，$\gamma<1$，为一个常量，称为折扣因子。

但是，在直复营销过程中，各个营销决策点之间的时间间隔是不确定的，比如相邻的两个营销时间有的间隔时间长，有的间隔时间比较短，所以就存在可变时间间隔的问题。如果采用公式$\eqref{seq:reward_3}$的方式计算累积奖赏，会带来一定的噪声影响，所以就要改变算法$\ref{algo:SVR+Q}$中的更新方式，以减少这种噪声给值函数的估计带来的影响。
如图$\ref{fig:2_ad_process}$所示。
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{2_ad_process}
\caption{广告在两个渠道上的生命周期}
\label{fig:2_ad_process}
\end{figure}

和标准的马尔科夫决策过程相比，在带有可变时间间隔的马尔科夫决策过程中，每次的交互事件都会被标记上时间。假设将初始状态的时刻记为$0$，那么后续交互事件被标记的时间都大于0。假设整个马尔科夫决策过程是从$t_{1}=0$时刻开始的，并且初始状态为$S_{1}$，之后agent重复的采取行为，便会得到一系列的由行为、状态、奖赏以及时间组成的四元组$\{<S_{i},A_{i},R_{i},t_{i}>\}_{i=1}^{\infty}$，其中$t_{i}$就是第$i$个时间发生的时间。那么，在计算累积折扣奖赏时，折扣因子就被定义为关于时间的函数，如式\eqref{seq:r}所示：
\begin{equation}\label{seq:r}
\begin{aligned}
G_{t}=\sum_{i=1}^{\infty} \gamma^{t_{i}}R_{i}
\end{aligned}
\end{equation}

在可变时间间隔的马尔科夫决策过程中，需要为可变长时间间隔中的奖赏找到一个合适的标准化方法，以减少可变时间间隔给奖赏的计算所带来的噪声。通常的做法是采用将一个时间间隔内收到的奖赏除以时间间隔长度的方式进行标准化。如\eqref{seq:r_1}所示：
\begin{equation}\label{seq:r_1}
\begin{aligned}
R_{i}^{'}=\frac{R_{i}}{t_{i+1}-t_{i}}=\frac{R_{i}}{\triangle t_{i}}
\end{aligned}
\end{equation}

在对原始奖赏值$R_{i}$进行标准化后，就要按照算法$\ref{algo:SVR+Q}$中第4行和第5行的方式进行更新样本，并利用样本得到奖赏的估计模型。那么，此时更新值函数的时候，第11行的更新公式就要按照如下式\eqref{seq:r_2}的方式进行更新：
\begin{equation}\label{seq:r_2}
\begin{aligned}
v_{i,j}^{(p)}=&(1-\alpha^{(p)})Q^{(p-1)}(S_{i,j},A_{i,j}) \cdot \triangle t_{i,j} \\
&+ \alpha^{(p)} (R_{i,j} + \gamma^{\triangle t_{i,j}} \max_{a} Q^{(p-1)}(S_{i,j+1,},a) \cdot \triangle t_{i,j+1})
\end{aligned}
\end{equation}

从式\eqref{seq:r_2}可以看到，时间间隔$\triangle t_{i,j}$在更新的时候，会受到学习率$\alpha^{(p)}$的影响。比如，随着学习率的的升高，时间间隔的值也在增加，就相当于给值函数的更新赋予更大的权重。所以，在更新Q值函数$v_{i,j}$的时候，还要考虑到对时间间隔的更新。

因为考虑到上述问题产生的原因主要是受学习率的影响，所以本文基于时间间隔构建一个标准化因子，并仿照Q值函数的更新方式进行标准化因子的更新，以解决之前更新方式所带来的偏差影响。设标准化因子为$Z_{i,j}$，那么其更新方式可表示为式\eqref{seq:r_3}
\begin{equation}\label{seq:r_3}
\begin{aligned}
Z^{(p)}_{i,j}=(1-\alpha^{(p)})Z^{(p-1)}_{i,j}+\alpha^{(p)}(\triangle t_{i,j} + \gamma^{\triangle t_{i,j}} \cdot Z^{(p-1)}_{i,j+1})\;
\end{aligned}
\end{equation}

由此，本节得到基于可变时间间隔的SVR+Q的算法如$\ref{algo:RBF-SVR-Q-t}$所示。其中，第1行到第5行进行时间间隔的计算，第6行到第12行进行标准化因子以及样本的的初始化，第18行进行Q值函数的更新，此处对应时刻点的Q值函数要乘以对应时刻点的归一化因子，第20行进行归一化因子的更新。

\subsection{改进的采样方法}
在直复营销等类似的应用场景中，产生的数据量非常大，如果将全部样本都导入逼近模型中进行训练，将必然会加重模型的训练负担，另外，Q值函数的更新也会消耗很多的时间。所以，为了解决以上问题，应该采取有效的采样方法来加快模型的训练和更新速度。
\begin{algorithm}[htbp]
\small
\SetAlgoLined
\SetKwRepeat{Repeat}{repeat}{until} 
\KwData{折扣因子$\gamma$，正则化参数$C$，RBF核函数的宽度$\sigma$，最大迭代次数$P$，原始样本$D=\{e_{i}|i=1,\cdots,I\}$，$e_{i}=\{<S_{i,j}, A_{i,j}, R_{i,j}, t_{i,j}>|j=1,\cdots,l_{i}\}$，（$D_{i}$表示第$i$个渠道的样本，$e_{i,j}$表示第$i$个渠道第$j$个情节，$l_{i,j}$为$e_{i,j}$的长度）}

\KwResult{输出最终的逼近模型：$Q^{(P)}$}

\For{all $e_{i} \in D$}{
	\For{$j$ \KwTo $l_{i}$}{
		计算时间间隔，$\triangle t_{i,j}=t_{i,j+1}-t_{i,j}$\;
	}
}
\For{all $e_{i} \in D$}{
	\For{$j$ \KwTo $l_{i}-1$}{
		得到初始的标准化因子：$Z^{(0)}_{i,j}=\triangle t_{i,j}$\;
		得到初始的状态值：$v^{(0)}_{i,j} = R_{i,j}$\;
		将初始的状态值使用标准化因子进行标准化：$D_{i}^{(0)}=\{<S_{i,j}, A_{i,j}, \frac{v^{(0)}_{i,j}}{Z^{(0)}_{i,j}}>|j=1,\cdots,l_{i}\}$\;
	}
}

同算法$\ref{algo:SVR+Q}$中第4行到第6行\;
% 整合所有情节的数据，生成总样本集合：$D^{(0)}=\cup_{i=1,\cdots,I} D_{i}^{(0)}$\;
% 将总样本集合$D^{(0)}$，按照行为标签ID$(A_{i,j}) \in \{1,2,\cdots, K \}$分发到各子模型的样本集合中，并利用SVR+Q模型的公式$\eqref{seq_final}$分别逼近值函数：$Q_{k}^{(0)}$， $k=1,\cdots,K$\;
% 多路逼近Q值函数：$Q^{(0)}=\cup_{k=1,\cdots,K}，Q_{k}^{(0)}$\;

\For{$p=1$ \KwTo $P$}{
	\For{all $e_{i} \in D$}{
		\For{$j$ \KwTo $l_{i}-1$}{
			更新Q值函数\;
			$v_{i,j}^{(p)}=(1-\alpha^{(p)})Q^{(p-1)}(S_{i,j},A_{i,j}) \cdot \triangle  Z_{i,j}^{(p-1)} 
			+ \alpha^{(p)} (R_{i,j} + \gamma^{\triangle t_{i,j}} \max_{a} Q^{(p-1)}(S_{i,j+1,},a) \cdot \triangle  Z_{i,j+1}^{(p-1)} )$\;
			更新标准化因子\;
			$Z^{(p)}_{i,j}=(1-\alpha^{(p)})Z^{(p-1)}_{i,j}+\alpha^{(p)}(\triangle t_{i,j} + \gamma^{\triangle t_{i,j}} \cdot Z^{(p-1)}_{i,j+1})$\;
			% 更新第$j$个情节的样本：$D_{i,j}^{(p)}=\{<S_{i,j,k}, A_{i,j,k}, \frac{v_{i,j,k}^{(p)}}{Z^{(p)_{i,j,k}}}>|k=1,\cdots,l_{i,j}-1\}$\;
		}
		更新第$i$个情节的样本：$D_{i}^{(p)}=\{<S_{i,j}, A_{i,j}, \frac{v_{i,j}^{(p)}}{Z^{(p)_{i,j}}}>|j=1,\cdots,l_{i}-1\}$\;
	}
	% 整合所有情节的数据，生成总样本集合。$D^{(p)}=\cup_{i=1,\cdots,I} D_{i}^{(p)}$\;
	% 将总样本集合$D^{(p)}$，按照行为标签ID$(A_{i,j}) \in \{1,2,\cdots, K \}$ 分发到各子模型的样本集合中，并利用SVR+Q模型的公式$\eqref{seq_final}$分别逼近值函数：$Q_{k}^{(p)}$，$k=1,\cdots,K$\;
	% 多路逼近Q值函数：$Q^{(p)}=\cup_{k=1,\cdots,K} Q_{i}^{(p)}$\;
	同算法$\ref{algo:SVR+Q}$中第15行到第17行\;
}
输出最终的逼近值函数$Q^{(P)} = Q^{(P)} \cdot Z^{(P)}$\;
\caption{基于可变时间间隔的SVR+Q算法}
\label{algo:RBF-SVR-Q-t}
\end{algorithm}

在强化学习中，通常的采样方法主要有随机采样法和Q-采样法，随机采样法，就是在每次从数据集中取情节的时候（如算法$\ref{algo:RBF-SVR-Q-t}$中，第6行和第15行），并不是取所有的情节，而是随机选取部分的情节，这种随机采样方法虽然可以减轻模型的训练和值函数的更新时间，但是采样后的数据质量并不高，因而会影响最后值函数的逼近效果。Q-采样法，在随机采样的基础上，当进行Q值函数的更新时，不是每个情节中所有事件都会被选择用于值函数的更新，而只有利用当前的估计Q值函数，可以在下一个状态中取得最佳行为的状态才能被选择。即在算法$\ref{algo:RBF-SVR-Q-t}$的第11到21行替换成如下算法$\ref{algo:SVR+Q_}$的语句：

\begin{algorithm}[htbp]
\small
\SetAlgoLined
\SetKwIF{If}{ElseIf}{Else}{if}{then}{else if}{else}{endif}
从数据集合$D$中随机采样一个样本子集，$R^{(p)}$\;
\For{all $e_{i} \in R^{(p)}$}{
	\For{$j$ \KwTo $l_{i}-1$}{
		\If{$Q^{(p-1)}(S_{i,j+1},A_{i,j+1})=\max_{a}Q^{(p-1)}(S_{i,j+1},a)$}{
			$v_{i,j}^{(p)}=(1-\alpha^{(p)})Q^{(p-1)}(S_{i,j},A_{i,j}) \cdot Z_{i,j}^{(p-1)} 
			+ \alpha^{(p)} (R_{i,j} + \gamma^{\triangle t_{i,j}} \max_{a} Q^{(p-1)}(S_{i,j+1,},a) \cdot  Z_{i,j+1}^{(p-1)} )$\;
			$Z^{(p)}_{i,j}=(1-\alpha^{(p)})Z^{(p-1)}_{i,j}+\alpha^{(p)}(\triangle t_{i,j} + \gamma^{\triangle t_{i,j}} \cdot Z^{(p-1)}_{i,j+1})$\;
		}
		更新第$i$个情节的样本：$D_{i}^{(p)}=D_{i}^{(p)} \cup \{<S_{i,j}, A_{i,j}, v_{i,j}^{(p)}>\}$\;
	}
}
\caption{基于Q采样的算法}
\label{algo:SVR+Q_}
\end{algorithm}

但是，通过Q采样的方法进行采样后的样本仍然很大，而且，即使选择了在下一状态中可以达到最佳行为的当前状态，但是该状态并不一定会有很高的学习效率。

考虑在本文第二章中，由公式$\eqref{shijiachafen}$计算出的$t$时刻的时间差分TD：$\delta_{t}$，应用在算法$\ref{algo:SVR+Q_}$的Q值函数中，可表示为式$\eqref{shijiachafenq}$：
\begin{equation}\label{shijiachafenq}
\begin{aligned}
&\delta_{i,j}=R_{i,j} + \gamma^{\triangle t_{i,j}} Q^{(p-1)}(S_{i,j+1}, a) \cdot Z_{i,j+1}^{(p-1)}  - Q^{(p-1)}(S_{i,j},A_{i,j}) \cdot Z_{i,j}^{(p-1)} \\
&\text{其中，} a=\argmax_{a} (Q^{(p-1)}(S_{i,j+1},a) \cdot Z_{i,j+1}^{(p-1)})
\end{aligned}
\end{equation}

式$\eqref{shijiachafenq}$中，$R_{i,j} + \gamma^{\triangle t_{i,j}} Q^{(p-1)}(S_{i,j+1}, a) \cdot Z_{i,j+1}^{(p-1)}$称为TD目标，即值函数逼近的目标值。如果TD偏差$\delta_{t}$越大，说明该状态处的值函数$Q^{(p-1)}(S_{i,j},A_{i,j}) \cdot Z_{i,j}^{(p-1)}$与TD目标目标的差距越大，进一步说明agent的更新量越大，因此在该处的学习效率就越高。所以，在抽样的时候，可以根据TD偏差的大小再一次进行有选择的采样。具体地，当使用Q采样算法$\ref{algo:SVR+Q_}$第4行条件进行初步选择后，设定一个固定的阈值$\eta$，然后使用公式$\eqref{shijiachafenq}$再一次进行选择，只选择TD偏差大于$\eta$的样本进行更新。所以，算法$\ref{algo:SVR+Q_}$可以修改为$\ref{algo:SVR+Q_2}$的形式。

\begin{algorithm}[htbp]
\small
\SetAlgoLined
\SetKwIF{If}{ElseIf}{Else}{if}{then}{else if}{else}{endif}
\KwData{增加一个TD偏差阈值$\eta$}
从数据集合$D$中随机采样一个样本子集，$R^{(p)}$\;
\For{all $e_{i} \in R_{k}$}{
	\For{$j$ \KwTo $l_{i}-1$}{
		\If{$Q^{(p-1)}(S_{i,j+1},A_{i,j+1})=\max_{a}Q^{(p-1)}(S_{i,j+1},a)$ $\bm{And}$
			利用公式$\eqref{shijiachafenq}$计算的$\delta_{i,j} \geqslant$  $\eta$
		}{
			$v_{i,j}^{(p)}=(1-\alpha^{(p)})Q^{(p-1)}(S_{i,j},A_{i,j}) \cdot Z_{i,j}^{(p-1)} 
			+ \alpha^{(p)} (R_{i,j} + \gamma^{\triangle t_{i,j}} \max_{a} Q^{(p-1)}(S_{i,j+1,},a) \cdot  Z_{i,j+1}^{(p-1)} )$\;
			$Z^{(p)}_{i,j}=(1-\alpha^{(p)})Z^{(p-1)}_{i,j}+\alpha^{(p)}(\triangle t_{i,j} + \gamma^{\triangle t_{i,j}} \cdot Z^{(p-1)}_{i,j+1})$\;
		}
		更新第$i$个情节的样本：$D_{i}^{(p)}=D_{i}^{(p)} \cup \{<S_{i,j}, A_{i,j}, v_{i,j}^{(p)}>\}$\;
	}
}
\caption{基于TD偏差的Q采样算法}
\label{algo:SVR+Q_2}
\end{algorithm}

\section{仿真实验}
本节，借助直复营销场景中的直邮营销数据集，通过仿真的方法进行模型的评估。首先，对实验中所使用的数据集进行介绍，并详细说明所选择的特征信息；然后，介绍本实验中的仿真评估方法；最后，介绍仿真试验的结果，并对结果进行分析。

\subsection{数据集}
本文选择使用UCI数据库中关于直邮营销著名的公开数据集KDD-CUP 1998\footnote{https://kdd.ics.uci.edu/databases/kddcup98/kddcup98.html}。该数据集有两个用途，一方面，利用数据集中的数据进行模型的训练，生成营销策略。另一方面，利用该数据集构建马尔科夫决策过程的仿真模型，然后利用这个仿真模型进行运行仿真试验，以评估不同模型输出的营销策略。

\paragraph{数据集背景}
直邮营销是直复营销中最早的、也是最为经典的应用场景，它是通过对目标客户以邮寄营销信件的方式达到营销宣传的目的。KDD-CUP 1998数据集是由美国非盈利组织PVA（Paralyzed Veterans of America）收集的，该组织通过直邮的方式向潜在捐助者发布捐助活动信息以
筹集资金，为有脊髓损伤疾病的美国退伍军人提供援助。

该数据集由在直邮捐助活动过程中所产生的信息组成。共搜集了95412名捐助者的信息，每条信息包括捐助者的个人基本信息以及在两年间所进行的22次募捐活动的历史营销纪录，这些记录包括：该是否向该捐助者进行了邮寄营销、该捐助者是否产生发生了捐助、捐助了多少金额等信息，另外邮寄的时间以及捐助者的回复时间都是可获得的。
% 其中23个捐助活动中又含有11种不同类型的邮件。
有关数据集的其它描述信息可在UCI网站上获得。

\paragraph{特征选择}
首先，需要确定模型中的数据结构。在批强化学习中，数据是以情节（episode）的形式存在的，每个情节内有包含一系列有序的事件（event），每个事件是由状态，行为和奖赏所构成的三元组。因此，在KDD-CUP 1998数据集中，可以将每个捐助者产生的22次营销数据看作一个情节，每次营销看作一个事件。那么，每个捐助者就对应一个情节，每个情节里有22个事件，且22个事件是按照真实的营销时间有序排开的。特别地，每个事件里包括该捐助者此时的状态，PVA对其采取的营销行为以及此次营销捐助的金额。

为了能够准确描述捐助者在每一次营销活动时的状态，我们需要借助专家领域知识，选取高效的特征。本节参考文献\citep{pednault2002sequential}中的特征选取方法，进行了捐助者特征的提取。特别地，在KDD-CUP 1998数据集中针对每个捐助者的基本信息，我们只选择了年龄和收入两个指标。另外，根据根据数据集中的营销交互信息，生成了许多临时特征。如表~\ref{tab:obser_donors}所示。其中，倒数第二条的action属性对应事件中的行为信息，倒数第一条的money属性减去营销的成本就是对应事件中的奖赏信息，其余特征对应事件中的捐助者状态信息。特别地，为了更好的捕捉到捐助者的状态，本节使用了一个六个月时间的历史窗口来提取捐助者的近期状态特征，那么，前六个月的数据就是第一个历史窗口信息，所以，一个情节中22个事件中就只剩下了16个

需要注意的是，表~\ref{tab:obser_donors}中的大部分特征都是无法从数据集中直接得到的，而需要通过遍历数据计算统计得到。比如，在数据集中，每个捐助者的numprom字段只有一个值，它表示的意思是最后一次营销（第22次）结束后向该捐者发送直邮营销的次数 ，那么我们为了得到在每一次营销时的捐助者信息，应该从后往前进行遍历，每当在直邮营销活动中发现PVA对该捐助者进行了邮寄，就进行减一操作。同样，对于ngiftall字段，每个捐助者也只有一个值，它表示的意思时最后一次营销结束后该捐助者捐助的次数，而我们为了得到每一次营销时的捐助者信息，应该从后往前进行遍历，每当在营销活动中发现该捐助者进行了捐助就进行减一操作。其他的特征也都需要进行类似的统计计算，以得到捐助者在每个营销点的状态信息。

\begin{table}[htbp]
  \centering
  \caption{捐助者特征的选取}
  \label{tab:obser_donors}
  \begin{tabular}{cl}
    \toprule
      特征 & 描述 \\
    \midrule
      age & 捐助者的年龄 \\
      income & 捐助者的收入 \\
      ngiftall & 捐助者的捐助的次数 \\
      numprom & 向该捐者发送直邮营销的次数 \\
      frequency & 该捐助者捐助的频率：numgiftall／numprom \\
      ramntall & 捐助的总金额 \\
      recency & 距离上次捐助的时间 \\
      promrecency & 距离上次发送直邮营销的时间\\
      timelag & 第一次直邮营销和第一次捐助的时间\\
      recencyratio & recency / timelag\\
	  promrecratio & promrecency / timelag\\
      nrecproms & 之前六个月里，给该捐者者发送直邮营销的次数\\
      nrecgifts & 之前六个月里，捐助者捐助的次数\\
      totrecamt & 之前六个月里，捐助者捐助的金额之和\\            	  
	  action & PVA是否进行了直邮营销（0，1）\\
	  money &  捐助的金额\\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{仿真评估方法}
评估强化学习模型的好坏需要有稳定的仿真环境。目前，OPENAI提供了许多游戏的仿真环境开源库，科研人员可以通过其提供的API接口非常方便的调用这些仿真环境，来训练、测试自己模型，所以，这也是目前大多数强化学习的研究者选择在游戏领域进行研究的一个重要方面。然而，在其它的一些应用中，需要研究人员根据训练数据自己构建仿真环境。本节，针对KDD-CUP 1998数据集，按照文献\citep{pednault2002sequential}提供的方法，通过构建关于数据集的马尔科夫决策过程来搭建仿真环境，进而评价模型输出策略的好坏。

\paragraph{构建MDP模型}
构建马尔科夫决策过程主要由两个评估模型组成：第一个模型$P(s,a)$是用来预测捐助者的产生募捐的响应概率，它是一个关于状态$s$和行为$a$的函数。第二个模型$A(s,a)$用来预测如果募捐者对此次募捐邮件产生了响应，会产生多少金额的募捐。其中，在本节的实验中模型$P(s,a)$和$A(s,a)$分别使用XGboost分类模型和XGboost回归模型进行构建。

当有了模型$P(s,a)$和$A(s,a)$，我们可以使用如下过程来构建马尔科夫决策过程。

（1）给定状态$s$和行为$a$下的即时奖赏$R(s,a)$可以通过这两个模型得到：以$P(s,a)$的概率来掷一枚硬币，并以此来判断捐助者是否会产生反馈。即：出现正面的概率为$P(s,a)$表示捐助者会产生反馈，出现反面的概率为$1-P(s,a)$表示捐助者忽略掉了本次募捐邮件，不会产生任何反馈信息。如果没有出现反馈，那么捐助额为$0$，如果产生了反馈，那么用户的捐助金额为$A(s,a)$。即时奖赏$R(s,a)$等于捐助额减去邮寄的成本。

（2）状态转移方程也可以通过使用这两个模型计算每一个状态特征的变化情况而得到。比如：如果PVA采取的行动为1，numprom的值就会加1，否则保持不变；同样的，如果上述掷硬币的方式出现正面朝上，那么ngiftall的值就会加1，否则保持不变；有了这两个值（numprom，ngiftall），那么frequency的值也就可以计算出来了。类似的，其他状态特征也是通过这种方法计算的到的。

\paragraph{评估方法}
有了上述马尔科夫决策过程的模型和状态转移方程，就可以使用如下的方式来构建我们的评估实验。

（1）首先，随机选择一定数量规模（5000）的捐助者，并设置他们的初始状态。本实验中将所选择的捐助者的初始状态设置为第7个营销活动时的状态。

（2）然后，使用学习好的强化学习模型输出策略：对每个捐助者是否应该采取营销行为。

（3）最后，使用模型$P(s,a)$和$A(s,a)$，可以得到预估的即时奖赏以及下一时刻的状态。记录这些得到信息，然后进入下一次的邮寄决策中。

就这样，按照上述的三个步骤，每循环一次就会模拟一次虚拟的直邮营销的过程。本节实验中，重复循环20次，就得到了20次的模拟直邮效果。

在文献\citep{pednault2002sequential}中作者提出，构建上述仿真环境的假设前提是认为募捐者的交互过程是一个马尔科夫决策过程，但是，在现实中这不一定是合适的。作者认为，像直复营销这种与人类行为有关的场景都过于复杂，使用简单的马尔科夫决策过程是无法较好的捕捉到环境的变化规律的，但是，我们使用这个仿真环境只是去评估我们的模型所产生的策略的好坏，只是在真实场景应用前的一个评估实验。所以，这种评估方法是可接受的，也是目前强化学习评估实验所普遍采用的方法。

\subsection{仿真结果}
下面，本节将分别从模型的长期收益、输出策略的变化情况以及不同采样方法下模型的效果对比等三方面展开对模型的评估。具体地，在比较长期收益以及策略的变化情况时，我们将基于可变时间间隔的Q-learning（IntervalQ）方法与文献\citep{pednault2002sequential}中的batch Q-learning算法以及本章的多元线性回归模型（监督学习方法，SL）进行对比。然后，从样本采样数量、长期收益等两方面来比较随机采样、Q采样方法以及本章所提的改进Q采样方法。

特别地，使用SL模型在训练时，即学习在给定状态行为下的即刻奖赏。在评估时，将训练好的多元线性回归模型用于预测在给定状态下（不）采取行为的奖赏值，如果采取营销行为获得的奖赏值大于不采取营销行为获得的奖赏值，那么就进行营销邮件的发送，否则不发送营销邮件。

另外，如上文所属，在进行仿真评估的时候，本文选择的评估样本规模为5000个，他们的初值状态对应第7次营销活动时的状态。

\paragraph{长期收益}
首先，本节考察IntervalQ模型，batch Q-learning模型以及SL模型在20个虚拟营销仿真中的总收益。一方面证明强化学习的方法比监督学习的方法在序列化决策问题上会有更好的收益，另一方面期望证明基于可变时间间隔的ntervalQ模型在直复营销场景中比普通batch Q-learning模型有更好的表现。

在本小节的实验中，每次试验选取10,000个情节进行训练，也就是160,000条事件数据，每次试验的最大迭代次数为10，当每次迭代结束后利用仿真环境进行评估实验，输出20个虚拟营销的总利润。每组实验都运行5次，并取5次试验中每次迭代结果的平均值作为最后的输出值，如图（）所示。其中，误差线表示五次试验中的总利润的标准误差。从图中可以看出，IntervalQ和batch Q-learning比SL模型具有显著的长期收益，而且因为考虑到了可变时间间隔的影响，IntervalQ在第4轮迭代以后比batch Q-learningyou很好的表现。从模型的稳定性上看，提出的IntervalQ的稳定性也好于batch Q-learning算法。


\paragraph{策略的变化}
接着，考察以上三个模型得到的20个营销策略是如何变化的。图（）显示了第10次迭代输出的20个营销活动中每次营销邮寄的人数。同样，在这个实验中，仍然选择5000个募捐者作为营销对象，从图中可以看出，通过IntervalQ和batch Q-learning算法在5个营销活动后有更好的成本控制能力，特别地，通过强化学习产生的营销策略可以看出，它最初邮寄给比较多的目标对象，等待观察回应，然后又再次尝试发给较多的目标对象，通过这种不断的尝试理解募捐者的交互行为，可以使的在序列决策上作出最优的策略。

进一步，本节考察随着营销时间的推移，每一次营销所产生的利润是如何变化的。图（）显示了第10次迭代输出的20次营销策略的所获的利润。从图中，可以清楚的看出，强化学习方法产生的营销策略最初获得利润较低，但是在之后的营销活动中，其获得的利润在不断提升这表明考虑到长期影响的强化学习方法是以最大化长期价值为目标，而不是为了获得短暂的即时收益。

还要注意的是，尽管第二次营销期间所发送的邮件数量很少，但是仍然有很大的利润。这些利润代表了先前营销活动的延迟回应，所以，该利润被记录到了他们收到捐款的时间而不是进行营销的时间。所以，从这一方面也可以看出，为了可以准确的在马尔科夫决策过程中作出序列决策，应该考虑到延迟影响的信用分配问题。

\paragraph{采样方法比较}
最后，从输出策略的质量和采样的数据集大小来验证本节提出采样方法的有效性。图（）展示了随着迭代次数的增大，不同采样方法所获得的长期利润。主要是将本章提出的改进Q采样方法和Q采样方法以及随机采样方法的对比。图（）展示了随着迭代次数的变化，不同采样方法的采样规模是如何变化饿。在这三个实验中，使用了大小为10,000的情节数据集，其中每次迭代时又5,000个被随机抽样，也就是80,000个事件数据。从图中可以看出，改进的Q采样方法相比Q采样方法进一步降低了样本数量。

另外，比较这两幅图清楚地表明了所提出的抽样方法的优点。通过改进的Q采样方法，不仅不会因为减少样本而使的所获的长期利润变小，反而可以产生更多的收益，侧面反正了这种采样方法可以提高样本的的学习效率。因为该实验中的行为数只有两个，而在许多实际应用中，可能会有大量可能的行为可供选择。在这种情况下，人们可以期望在计算资源方面有相当大的节省。

\section{本章小结}
