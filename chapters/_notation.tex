\chapter{主要符号表}

\begin{longtable}{p{2cm}p{11cm}}
% \label{tab:test}
 \toprule
符号 & 解释 \\
 \midrule
\endfirsthead
 % {\bf 续表~\ref{tab:test}}\\
 {\bf 续表}\\
 \toprule
符号 & 解释 \\
 \midrule
\endhead
\endfoot
 \bottomrule
\endlastfoot
$s$，$s^{'} $                  & 状态（state），下一时刻的状态           \\
$a$                            & 行为（action）                \\
$r$                           & 奖赏（reward）                  \\
$\mathcal{S}$                            & 状态集合\\
$\mathcal{A}$                            & 行为集合                             \\
$\mathcal{R}$                            & 奖赏集合                                    \\
                            &                                         \\
$t$                            & 离散的时间步                                    \\
$T$                            & 一个情节（episode）中的最后一步              \\
$A_{t}$                            & $t$时刻的行为值              \\
$S_{t}$                          & $t$时刻的状态值                    \\
$R_{t}$                           & $t$时刻的奖赏值                               \\
$G_{t}$                            & $t$时刻的回报（累计折扣奖赏）    \\
% $G_{t}^{(n)}$                            & $n-$步回报    \\
% $G_{t}^{\lambda}$                            & $\lambda$步回报    \\
                            &                        \\
$\pi$                            & 策略                                              \\
$\pi(s)$     & 在确定性策略$\pi$下，当状态为$s$时所采取的行为                 \\
$\pi(a|s)$      & 在随机性策略$\pi$下，当状态为$s$时，采取的行为$a$的概率                 \\
$p(s^{'}|s,a)$      & 当采取行为$a$后，状态从$s$转移到$s^{'}$的概率  \\
$r(s,a,s^{'})$      &   当采取行为$a$后，状态从$s$转移到$s^{'}$得到的即时奖赏\\
                            &                                         \\
$v_{\pi}(s)$     & 策略$\pi$下，状态$s$的值（期望回报）             \\
$v_{*}(s)$     & 最优策略下，状态$s$的值    \\
$q_{\pi}(s,a)$     & 策略$\pi$下，在状态$s$时采取行为$a$的值             \\
$q_{*}(s,a)$     & 最优策略下，在状态$s$时采取行为$a$的值    \\
$V_{t}(s)$     & $v_{\pi}$或者$v_{*}$的估计值             \\
$Q_{t}(s,a)$     & $q_{\pi}$或者$q_{*}$的估计值    \\
$\hat{v}(s,\bm{\theta})$     & 给定权重向量$\bm{\theta}$下，状态$s$的近似值         \\
$\hat{q}(s,a,\bm{\theta})$     & 给定权重向量$\bm{\theta}$下，状态行为对$s$，$a$的近似值     \\
                            &                                         \\

% $Z_{t}(s)$     &  在$t$时刻时，状态$s$的资格迹             \\
% $Z_{t}(s,a)$     & 在$t$时刻时，状态行为对$s$，$a$的资格迹    \\
% $\bm{z}_{t}$     & 在$t$时刻时，资格迹的向量    \\
                            % &                                         \\
$\mathbb{E}[X]$     & 随机变量$X$的期望值：$\mathbb{E}[X]=\sum_{x}p(x)x$    \\                      
$\mathbb{R}$     & 实数集    \\
$\text{Pr}\{X=x\}$     & 当变量$X$等于$x$的概率    \\
                            &                        \\
$\delta_{t}$     & 在$t$时刻的时间差分偏差值    \\
$\gamma$     & 累积奖赏的折扣率    \\
$\epsilon$     & 在$\epsilon-greddy$ 策略下，随机行为的选择概率         \\
$\alpha$     & 步长参数  \\

\end{longtable}


% % \usepackage{longtable}
% % \begin{longtable}{|l|l|l|}
%   % \centering
%   % \begin{tabular}{cl}
%   % \toprule
%  \hline
%  符号 & 解释 \\
%  \hline
% $s$，$s^{'} $                           & 状态（states）               \\
% $a$                            & 行为（action）                \\
% $r$                           & 奖赏（reward）                  \\
% $S$                            & 状态集合\\
% $A$                            & 行为集合                             \\
% $R$                            & 奖赏集合                                    \\
%                             &                                         \\
% $t$                            & 离散的时间步                                    \\
% $T$                            & 一个情节（episode）的最后一步              \\
% $A_{t}$                            & $t$时刻的行为值              \\
% $S_{t}$                          & $t$时刻的状态值                    \\
% $R_{t}$                           & $t$时刻的奖赏值                               \\
% $G_{t}$                            & $t$时刻之后的回报（累计折扣奖赏）    \\
% $G_{t}^{(n)}$                            & $n-$步回报    \\
% $G_{t}^{\lambda}$                            & $lambda$步回报    \\
%                             &                        \\
% $\pi$                            & 策略                                              \\
% $\pi(s)$     & 在确定性策略$\pi$下，当状态为$s$时所采取的行为                 \\
% $\pi(a|s)$      & 在随机性策略$\pi$下，当状态为$s$时，采取的行为$a$的概率                 \\
% $p(s_{'}|s,a)$      & 当采取行为$a$后，状态从$s$转移到$s_{'}$的概率  \\
% $r(s,a,s_{'})$      &   当采取行为$a$后，状态从$s$转移到$s_{'}$得到的即时奖赏\\
%                             &                                         \\
% $v_{\pi}(s)$     & 在策略$\pi$下，状态$s$的价值（期望回报）             \\
% $v_{*}(s)$     & 最优策略下，状态$s$的价值    \\
% $q_{\pi}(s)$     & 在策略$\pi$下，在状态$s$的采取行为$a$的价值             \\
% $v_{*}(s)$     & 最优策略下，在状态$s$的采取行为$a$价值    \\
% $V_{t}(s)$     & $v_{\pi}$或者$v_{*}$的估计             \\
% $Q_{t}(s)$     & $q_{\pi}$或者$q_{*}$的估计    \\
% $\hat{v}(s,\bm{w})$     & 给定权重向量$\bm{w}$下，状态$s$的近似值         \\
% $\hat{q}(s,a,\bm{w})$     & 给定权重向量$\bm{w}$下，状态行为对$s$，$a$的近似值     \\
%                             &                                         \\
% $\delta_{t}$     & 在$t$时刻的时间差分误差    \\
% $Z_{t}(s)$     &  在$t$时刻时，状态$s$的资格迹             \\
% $Z_{t}(s,a)$     & 在$t$时刻时，状态行为对$s$，$a$的资格迹    \\
% $\bm{z}_{t}$     & 在$t$时刻时，资格迹的向量    \\
%                             &                                         \\
% $\gamma$     & 折扣率因子    \\
% $\epsilon$     & 在$\epsilon-greddy$ 策略下，随机行为的选择概率         \\
% $\alpha$，$\beta$     & 步长参数  \\

%   \hline
%   % \end{tabular}
% \end{longtable}
