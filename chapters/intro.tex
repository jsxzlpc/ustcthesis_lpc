\chapter{绪论}
 % 简短的介绍

\section{研究背景及意义}
 % 1、CRM出现的背景、含义和意义（适当的拔高）、进而引出ltv最大化和广告投放
 % 2、强化学习的出现的背景以及为什么可以用于解决CRM问题，但是解决这个问题还有什么问题。
在中国改革开放和国际化进程的不断深入下，我们对市场、对营销、对管理的态度发生了从无知到有知，从漠视到重视的巨大转变。时至今日，我们发现我们好像从来没有与世界的脉搏如此的接近过，同样，我们的企业和国家也从来没有如此深切的感受到全球化的竞争是如此残酷。因为市场竞争的不断加剧，每个企业都在努力寻找着自己的核心竞争能力，以在竞争中取得优势，使企业不断的发展壮大。但是，近年来，信息技术的广泛使用，使得众多行业的产品在价格、质量和服务上的差异越来越小，如何在如此激烈的市场竞争中领先对手，给当前的商业研究提出了新的要求，客户关系管理（Customer Relationship Management, CRM）就是其中一个重要的研究方向。

CRM一词最初由Gartner Group\footnote{高德纳咨询公司，全球第一家信息技术研究和分析的公司，现已发展成一家独立的咨询公司}在1999年提出，但是因为商业场景的复杂性和广泛性，不同的学者和商业机构从不同角度给出了不同的定义。结合Gartner Group和CRMguru.com\footnote{http://crmguru.com/：提供CRM相关文章、业界信息以及专业人员讨论区的网站}等公司对CRM的解释，一种可接受的定义为：CRM是现代信息技术、经营理念和管理思想的结合体，它以信息技术为手段，对现实的和潜在的客户关系以及业务伙伴关系进行多渠道管理，形式一个自动化的解决方案，最终实现业务操作效益的提高和利润的增长\citep{客户关系管理丁建石}。
其中，Customer代表的意义较为广泛，它包括了现实已存在的和潜在的客户以及业务伙伴。

CRM的所涉及的研究内容非常广泛，研究方向也朝着多元化的趋势发展\citep{王广宇2013客户关系管理}。在过去的二十年里，随着互联网和数据库的广泛应用，各个行业、各个公司每天都会积累一定的商业数据，如何将这些原始数据进行实时的和更深层次的分析，以辅助或代替企业进行商业决策，成为近年来CRM领域最为活跃的研究内容。其中，最有代表性的方向是基于机器学习技术与CRM的融合，利用机器学习技术可以从数据中自动提取出其背后隐含信息，然后更好的利用这些信息进行预测、分析和决策，使得CRM的全部功能得以有效发挥。

强化学习（Reinforcement Learning, RL）技术是机器学习的重要组成部分，主要用于解决序贯决策问题。它通过智能体Agent不断地与环境进行交互，并从环境反馈的延迟回报中学习状态与行为之间的映射关系，以使得回报达到长期最大化，即通过不断的学习，可以输出使得长期收益最大化的策略，这与CRM所追求的“产品”生命周期价值最大化的目标不谋而合。强化学习目前主要应用于机器人、游戏等领域，并都取得了令人振奋的表现。但是，因为其在真实的应用中容易出现不收敛或收敛速度慢、环境状态部分可观察、仿真环境很难建立等问题，针对其它领域的研究和应用则相对甚少。这也是本文采用强化学习的出发点。

本文主要关注CRM领域的两个应用场景：

1）直复营销活动（Direct Marketing Campaigns）问题。直复营销是传统而又经典的CRM应用，在直复营销活动的推广中，销售人员需要根据顾客资料和购买历史记录等信息，预测顾客未来关于是否会发生购买行为的响应模型，以在真实的直销活动中提高顾客的购买响应概率和购买金额，目的是最大化顾客的生命周期价值。

2）多渠道广告投放预算优化（Budget Allocation among Multi-Channle Advertising）问题。因为在现在的商业社会中，如果单单根据企业自己获取的顾客信息进行产品的直复营销活动推广，效率和速度是非常慢的，已经无法满足现在激烈而残酷的商业竞争环境了。所以，出现了很多拥有众多优质广告渠道资源的广告代理机构（Ad-Agence），他们拥有大量而丰富的用户资源，可以帮助业快速进行新用户的获取和商品的推广。所以，多渠道广告投放预算优化需要解决给定总的固定预算的情况下，如何做出科学的投放策略（每个渠道应该投放的资金量）以最大化多渠道的长期收益。\citep{zhang2017multi}。因为我们关注的是借助Ad-Agence进行广告投放的问题，所以一般无法接触顾客，因此属于间接营销活动。如果单单依靠市场人员进行分析判断，其工作量是巨大的而且往往产生不了较好的决策或者很难持续产生较好的决策。所以，通过强化学习技术，可以在自动化产生科学决策的同时使得企业的收益达到长期最大化，就变的十分重要和关键了。

通过研究直复营销活动中顾客生命周期价值最大化的问题，建立企业同顾客之间的长期关系，帮助企业分析每一位顾客在每一阶段的特点，对是否应该向该顾客提供营销信息或者应该提供什么样的营销信息做出科学判断，进而可以创造出不断增加的忠诚客户和更大的盈利空间。通过研究多渠道广告投放的应用，可以帮助企业了解每一个投放渠道在每一个阶段对企业的价值，进而可以做出智能化的投放策略，最大化每个渠道的潜力。总之，研究CRM的相关应用，可以提升客户关系的管理水平、树立企业新型营销理念、降低企业成本、提高企业效益，进而全面提高企业的核心竞争力，因此针对CRM领域的研究具有重要的研究前景和社会价值，同时，针对强化学习的研究和改进并将拓宽其应用领域，对强化学习的发展也有着重要而深远的意义。

\section{国内外研究现状}

\subsection{CRM的研究现状}
% 介绍crm目前的研究方向，引出其与机器结合的方法，进而介绍机器学习技术在定向促销和多渠道广告投放中的应用
虽然CRM一词最早出现在1999年，但是相关理论的研究已有百余年之久。总的来看CRM大概经历了CRM理念萌芽时期、CRM应用雏形化时期、CRM产品实用化时期以及CRM智能化集成时期\citep{王广宇2013客户关系管理}，其中，CRM智能化集成就是指将人工智能等手段集成到CRM应用中，多年来，经过众多学者和专家的不断努力，已经在此方面取得了瞩目的成果\citep{bahari2015efficient,farquad2014churn,linoff2011data,chiang2018applying,ballings2015crm}，并将其应用到各种商用CRM系统中，比如国外微软公司的Dynamics CRM、甲骨文公司的Siebel、People Soft以及惠普公司的Front Office等CRM产品，国内主要有用友公司的TruboCRM产品，立有信公司的MyCRM产品等。由于本文主要关注直复营销活动和多渠道广告投放两个应用场景，因此主要将这两个方面的研究现状进行详细介绍。

\paragraph{直复营销活动}
目前，针对直复营销活动推广领域的研究，主要是用监督和非监督的机器学习方法。

国外关于直复营销推广的研究起步较早，并且有着诸如KDD-CUP-98这样有关直销算法策略的比赛以及丰富的数据集，几年来取得了众多研究成果。文献\citep{wong2005mining}针对直接营销活动存在的顾客购买的可能性与购买所花费的金额之间往往会呈现一定的负相关性的问题，提出直接估计客户所能产生的利润而不是估计客户发生购买的概率的方法，进而使用关联规则的方法用于构建预测未来客户价值的模型，得到了非常好的结结果，作者使用KDD-CUP-98作为数据集，得到的结果甚至比该比赛的冠军利润率高出了41％。文献\citep{karim2013decision}将朴素贝叶斯算法和C4.5决策树算法分别应用到银行的直销推广活动中，以此来预测客户是否会订购定期的理财产品，然后使用一个新的算法\citep{alam2012actionable}从预测结果中提出可行的营销行为知识，并在UCI公开数据集上对这两种算法的性能进行了对比，得出C4.5决策树算法在预测的准确度（Accuracy）、ROC值、精确度（Precision）等方面都好于朴素贝叶斯算法的结论。文献\citep{coussement2015improving}较为细致和全面的介绍了大部分的数据挖掘技术（逻辑回归、线性二次判别分析法、朴素贝叶斯、神经网络、决策树以及KNN算法）在四个直销数据集上的应用情况，并且通过权衡结果的可解释行与预测性能的准确性两个方面之间的关系，给出了如何进行算法选择的意见。作者通过实验发现，CHAID，CART和神经网络算法在测试集上的表现要好于其他的算法。文献\citep{parlar2017using}主要关注是的是特征选择，使用信息增益和卡方的方法来选择重要的特征，然后使用朴素贝叶斯算法来比较这两个特征选择的方法，实验表明，提出的方法在减少特征集的同时提高的分类性能。

国内关于直复营销的研究起步相对较晚，研究人员也相对较少，大多集中于营销理论的研究，而对基于机器学习技术的直复营销策略的研究相对较少。但是，近年来经过学者们的不断努力，在相关领域也取得了很多实质性的进展。文献\citep{徐笑昂2007直复营销模式下的商品选择和顾客分群算法研究}针对商品选择和顾客之间的密切关系，提出了一种顾客导向下基于交叉营销的双重分割算法DualSeg，主要适用关联规则挖掘商品与商品之间、顾客与顾客之间以及顾客与商品之间的影响，从而解决了直复营销中的商品选择和顾客分群问题。文献\citep{chen2015behavior}针对营销中存在异构型数据的特点，提出了一种改进的分层多核支持向量机模型（H-MK-SVM）用于感知用户的响应模型，该模型可以从众多异构数据中进行数据转换和特征提取，将异构高维多关系型的数据转换为以客户为中心的高阶张量型数据进而可以提取特征属性，通过使用真实的营销数据验证了该改进模型比其他模型（SVM、Adaboost等）在准确度上的优越性。文献\citep{sing2013data}提供了一个基于机器学习技术在直邮营销方面的通用框架，详细描述了框架中的各个流程，最后通过实验评估，证明了该框架降低降低时间开销的同时较少了企业的营销成本。需要特别提到的是在文献\citep{ngai2009application}中，作者详细总结了CRM领域的近千篇文章，其中涵盖了CRM四个不同的维度以及数据挖掘的七个方面技术，本文的研究结果表明，分类和关联模型是数据挖掘技术在CRM最常用的两种模型。

通过上述分析总结可以发现，在基于监督学习和非监督学习的方法中，大多采用了分类模型和关联模型对用户的行为数据进行分析。在非监督模型中主要是将用户进行聚类分析，以辅助进行营销决策。在监督学习中，主要是判断用户在下一时间点是否会发生购买行为或者发生的购买行为的概率，进而得出在下一时间点应该做出什么样的营销方案。但是，这些方法存在以下不足：1）在监督学习中，是假设已经对环境有了充分的探索，然后从给定的带标签的数据集中进行学习样本的模型和策略，这样就会使模型丧失自学习的能力，因为如果已有的数据集本身就无法得到最优的结果，那么算法始终都无法学习到最好的营销方案。2）基于监督学习和非监督学习模型只考虑了当前时刻的目标最大化。即采取什么样的营销行为，可以使的下一时刻的利润最大化。这样就仅仅考虑了即时利润（Immediate Profits）最大化，而忽视长期利润最大化，造成的影响是即使在这一时刻做出了目前看来是最优的营销方案，但是该方案可能会对之后更佳的营销方案产生潜在的负面的影响，从而影响了整体的利润最大化。而强化学习可以解决以上这两个方面的问题，因为强化学习的目标是学习状态和行为之间的映射关系，使的整个任务序列的目标达到最大化，也就是长期目标最大化。同时，在学习过程中通过对未知环境不断尝试、探索，以建立环境模型进而达到自学习的目的。因此，强化学习已经成为近年来解决CRM问题的研究热点方向。

文献\citep{pednault2002sequential}首次将强化学习技术用于解决直复营销问题中，使用批Q-learning（Batch Q-learning）算法学习营销策略，并且提出了一些采样方法，通过基于KDD-CUP-98数据集进行仿真实验表明，该模型在整个营销的生命价值收益上远高于比赛中的其他模型。文献\citep{silver2013concurrent}提出了一种基于时间差分学习的并行强化学习框架，可以并发的解决企业与客户之间的交互问题，并且通过模拟器分别在非自举（Non-bootstrappig）、非在线（Non-online）和非序列化（Non-sequential）三个方面进行了模型的评估，得到在高并发的序列化问题中，考虑从部分序列中进行自举、进行在线学习以及使用序列化的强化学习是很重要的结论。文献\citep{tkachenko2015autonomous}该模型使用了深度强化学习解决直复营销中的问题，即使用Q-learning训练一个深度神经网络来学习客户的状态和营销行为之间的关系，同时，该文章使用Recency-Frequency-Monetary指标参数化客户状态空间，实现了客户响应率和花费金额的显著提高。

\paragraph{多渠道广告投放预算分配}
多渠道广告投放的预算分配是一个资金分配的优化问题，在经济学、运筹学和管理科学等领域中一直以来就是一个经典的问题\citep{zhang2017multi}。与其他CRM应用场景的目标相同，多渠道广告投放预算分配问题也是为了追求长期收益最大化。文献\citep{yang2012budget}针对搜索广告资金优化问题，提出了一个分层的预算分配优化框架BOF，可以分配和调整广告的预算，其中位于第二层的广告层（Campaign Level），主要是用于解决预算在多渠道的分配问题，经过实验评估验证了此框架输出的投放策略的效果优于实际广告中常用的两种基准投放策略。文献\citep{ding2013multi}研究了带有预算约束的多臂老虎机问题（Multi-Armed Bandit Problem, MAB），提出了基于两种基于置信区间上界（Upper Confidence Bound, UCB）改进的方法，并理论证明了这两种算法的后悔界（Regret Bound）为$O(lnB)$（其中$B$代表预算金额）具有很好的学习能力，并且通过真实的广告模拟，验证了算法的有效性。文献\citep{boutilier2016budget}引入带预算约束的MDPs(BMDPs),通过权衡预算分配和期望收益之间的关系得到一个关于预算的函数，并证明该函数是非递减的凹函数，最后利用动态规划的方法求出该问题的解。文献\citep{zhang2017multi}提出将多渠道广告投放资金问题转化为权重未知的多选择背包问题（Multi-Choice Knapsack Problem, MCKP），然后又提出了两种权重阈值的搜索方法，从理论和仿真实验两方面都证明了该搜索方法的可靠性。

通过分析国内在广告投放预算分配方向的相关论文，可以发现，大多数的研究内容都是根据用户与广告直接的交互数据进行建模，其解决方法除了增加预算约束外，大体和直复营销的解决思路相同。但是，因为基于Ad-Agence的广告投放预算分配场景中存在的所能获得的数据量较少、限制条件较多、仿真环境难以建立等特点，目前针对该场景的研究和应用还没有得到充分的关注。

\subsection{强化学习研究现状}
% 介绍强化学习的发展、热点和未来发展趋势
此部分将分为强化学习的总体发展历程、研究热点以及强化学习的发展趋势等三个方面进行展开介绍。

\paragraph{发展阶段}
强化学习的发展过程大概可以分为三个阶段。

第一阶段是1998年以前，这一阶段形成了强化学习基本理论框架，学者们关注和发展的最多的算法是基于表格型的强化学习算法，包括值迭代和策略迭代。代表性的工作是强化学习鼻祖Richard S.Sutten编辑的专刊，标志着强化学习发展成为机器学习领域的一个分支。该专栏后整理成书，成为强化学习领域的第一本著作：Reinforcement Learning: An introdcution，，该书第一次系统而全面的介绍了强化学习的相关理论知识，至今仍然被广大的教育机构和强化学习爱好者作为学习强化学习的经典教材（该书最新电子版可在网上免费获取\footnote{http://incompleteideas.net/book/bookdraft2017nov5.pdf}）,此外，这期间还出现了强化学习代表性算法Q-learning\citep{watkins1992q}和Sarsa\citep{rummery1994line}。

第二阶段是1998年到2013年，这一阶段基于直接策略搜索的强化学习方法得到了深入研究和发展。自从RONALD J. WILLIAMS在论文\citep{williams1992simple}中提出对Reinforce算法直接对策略梯度进行估计后，出现了各种改进方法，如：GPOMDP\citep{baxter2001infinite}、PEGASUS\citep{neumann2005reinforcement}以及与值函数结合的Actor-Critic算法\citep{konda2000actor}等。

第三阶段是2013年以后，随着深度学习的发展，这一阶段出现了深度强化学习算法。代表性的工作是DeepMind团队提出了DQN(Deep Q Network)算法并将其成功应用在雅达利（Atari）游戏中\citep{mnih2013playing}，之后无数的学者对其进行了改进研究，但是
大部分都应用在机器人和游戏中。
其中，最具轰动性的事件当属在2016年和2017年，谷歌的AlphaGo利用深度强化学习技术连续两年分别击败了世界围棋冠军李世石和柯洁。

\paragraph{研究热点}
但是，目前强化学习在实际应用中仍然存在维度灾、收敛速度慢、时间信度分配等问题，其中维度灾是指在大空间和连续问题中，强化学习无法在有限的空间和时间内学习到一个合理的解决方案，而收敛速度慢又与强维度灾有着密切的关联，所以解决维度灾问题对强化学习的应用起到了十分重要的作用。近年来，众多研究者主要集中于利用函数逼近的方法解决此问题，而函数逼近方法又可以分为参数化函数逼近方法和非参数化函数逼近方法。

参数化函数逼近方法又可以分为线性函数逼近和非线性函数逼近两种方法。其中在线性函数逼近中，基函数的形式和参数个数需要提前制定，往往会限制函数的逼近能力。该方法最早是由Samuel在1967年提出，并将其应用于西洋跳棋的系统设计中\citep{samuel1959some}。1988年，Sutton提出将线性函数逼近法与带有资格迹的时间差分（Temporal Difference, TD）方法相结合，然后使用梯度下降求解近似值函数的方法\citep{sutton1988learning}后，掀起了线性函数逼近法研究的热潮，相继出现了最小二乘时间差分（Least Squares Temporal Difference, LSTD）算法\citep{bradtke1996linear}、离策略（Off-policy）函数逼近方法\citep{precup2001off}、以及梯度时间差分（Gradient Temporal Difference, GTD）学习算法\citep{sutton2009convergent}等，其中GTD解决了离策略TD学习算法的不稳定问题，且具有较低的时间负责度\citep{sutton2009convergent}。非线性函数逼近法的函数逼近器是关于参数的非线性函数，如基于神经网络函数逼近方法，虽然该方法具有很强的表征能力，但是容易陷入局部最优，且收敛性难以保证。1995年Bertsekas等利用前向神经网络逼近强化学习中的值函数，取得了相比线性逼近较好的结果，但是往往会出现不稳定不收敛的情况\citep{bertsekas1995neuro}，直到2015年和2015年DeepMind团队提出的DQN网络对训练过程进行了经验回放\citep{mnih2013playing}和单独设立目标网络\citep{mnih2015human}的的改进，通过打破数据之间的关联性，使的神经网络的训练过程收敛且稳定，并且在游戏中取得了令人振奋的表现。从此以后，彻底打开了大家研究深度强化学习热情。2015年DeepMind团队有提出了Double DQN模型，在该模型中为了克服Q-learning本身固有的缺点——过估计（Overestimate），采用动作的选择和动作的评估分别使用不同的值函数来实现。除此以外，深度Sarsa、A3C(Asynchronous Advantage Actor-Critic)、DDPG(Deep Deterministic Policy Gradient)等一些列有影响力深度强化学习模型相继被提出。

非参数化函数逼近法，并不是没有任何参数的函数逼近，而是指参数个数和基函数的形式并非固定，完全由样本决定，因此具有更大的灵活性。非参数化函数逼近模型主要由基于高斯过程和基于核方法的值函数逼近模型。基于核方法的研究相对较多，如基于核的强化学习函数逼近方法\citep{ormoneit2002kernel}、基于核的最小二乘TD方法（Kernel-based Least Squares TD, KLSTD）\citep{xu2005kernel}、基于最小二乘的策略迭代算法等（Kenel-based Least Squares Policy Iteration, KLSPI）\citep{xu2007kernel}。

\paragraph{发展趋势}
强化学习正在飞速发展，从当前的论文中可以初步判断强化学习的发展具有如下趋势：1）强化学习与深度学习的结合会更加紧密。2）强化学习与领域知识的结合更加紧密，特别是重塑回报函数方向。3）强化学习的理论分析会更加全面、具体，算法性能会更加稳定和高效。

\section{主要研究内容}
首先，本文针对在CRM领域中直复营销存在的环境部分可观察的问题，提出了基于循环神经网络的深度强化学习模型。然后针，对多渠道广告资金分配问题中存在的数据量少、复杂度高以及非参数函数逼近模型易存在收敛速度慢的问题提出了基于RBF-SVR的分片逼近模型，同时又将该模型与MCKP结合，形成ML-MCKP模型。最后，针对两个模型进行仿真实验，证明了模型的有效性。主要研究内容包括以下三部分：

（1）在直复营销场景中，因为环境状态部分可观察，所以容易对函数逼近造成影响进而影响了强化学习的效果。针对此问题，本文提出了基于RNN的深度强化学习混合模型，该混合模型由两个网络构成，其中通过RNN网络从监督数据中学习到环境隐状态的表示后，再将隐状态作为DQN网络的输入，共同地优化控制以使得长期收益最大化。另外，网络参数训练分为两个阶段，在第一阶段时，RNN网络和DQN网络独立的学习各自的参数，互不影响，经过一定的训练次数时，第二阶段将两个网络连接起来，一起训练参数。与其它深度强化学习相比，该模型在第一阶段没有直接将观测的环境状态作为强化学习的输入，而是将预测的隐状态作为输入。通过这种改进的网络结构可以在不需要借助领域知识的情况下，提高值函数的学习精度和速度。

（2）在多渠道广告投放预算分配的场景中，存在数据量少、复杂度高等问题，容易对函数逼近造成影响进而影响了强化学习的效果。针对此问题，本文首先提出了非参数化函数分片逼近的方法解决该问题，以提高样本的使用效率和收敛精度。但是，针对非参数化函数逼近中容易出现收敛速度慢的问题，又提出了RBF-SVR模型，该模型可以将强化学习中的值函数逼近问题转化为高维空间中的线性回归问题，从而达到提高收敛速度的目的。另外，本文首次将强化学习和MCKP问题结合，完美的解决了给定预算下的资金分配问题，最后为了更好的解决现实问题，又提出了有效的花费空间离散方法、改进值函数表示方法以及有效的行为探索方法。

（3）在强化学习应用中建立仿真环境是很难的。针对此问题，按照文献\citep{pednault2002sequential}中仿真环境的建立方法，建立了直邮营销中的仿真环境，并且在此基础上，提出了建立了多渠道广告投放的仿真环境的方法。最后，在这两个仿真环境中验证了模型的有效性。

\section{本文结构安排}
文本围绕强化学习技术在CRM领域中的部分应用展开研究，具体的组织结构如下：

第一章 绪论。本章首先介绍了本文的研究背景，包括CRM问题和强化学习技术，并且引出了本文的选题和意义。接着，介绍了CRM和强化学习的研究现状，主要包括本文所选择的直复营销和多渠道广告投放两个领域的研究现状以及强化学习的发展阶段、研究热点和发展趋势，然后阐明了本文的研究内容及贡献，最后介绍了本文的组织结构。

第二章 相关理论知识。
