\chapter{总结与展望}

\section{本文工作总结}
针对传统的经典分类算法和基于代价敏感的分类算法在处理直复营销问题时，只能考虑单独事件的即时收益，而无法考虑连续决策的长期收益问题，本文选择使用强化学习的方法进行研究。然后，为了更好的适应实际需求，本文针对直复营销场景中的营销决策点不固定、数据负载大学习效率不高以及客户状态的部分可观测性等三个方面对基于值函数的强化学习算法进行改进，并使用仿真环境验证所提方法的有效性。本文的主要工作可以总结为以下三点。

1）关于传统值函数强化学习Q-learning的研究。基于经典的强化学习算法Q-learning进行研究，结合直复营销场景中决策点间可变时间间隔问题以及数据规模大学习效率不高的问题，提出了改进的Q-learning算法。具体地，首先，为了减少不同决策点之间的可变时间间隔给强化学习中的奖赏信号带来的噪声影响，提出使用均值归一化的方法进行解决。接着，针对Q值函数在迭代更新过程中因为时间间隔更新不同步问题而带来的偏差影响，提出一个标准化因子，并仿照值函数更新方法进行标准化因子的更新，从而可以有效的解决以上偏差问题。最后，为了解决在直复营销场景中，随着数据量的提升，Q-learning算法更新速度慢，学习效率不高的问题，在Q-采样的基础上，引入TD偏差，提出基于TD偏差的Q-采样方法，以减少训练次数的同时提高学习效果。

2）关于深度强化学习模型DQN的研究。基于深度强化学习DQN模型进行研究，针对传统强化学习Q-learning在处理直复营销场景中的客户状态的部分可观测问题时，需要引入大量专家领域知识的问题，提出了基于RNN的深度强化学习混合模型。具体地，首先，结合直复营销场景的时序特点，提出使用基于RNN的DQN模型（DQN_RNN）来解决直复营销的决策问题。然后，指出DQN_RNN模型在网络优化过程中不能很好的同时处理隐状态的学习和值函数的逼近，并由此提出基于两个网络的混合模型：通过RNN网络从监督数据中学习到隐状态的表示方法后，再将隐状态作为DQN网络的状态输入进行强化学习，通过这种方式可以使的两个网络优势互补，在达到值函数逼近效果的同时也跟好地学习到了隐状态的表示方法，从而摆脱了需要借助专家领域知识的困扰。最后，为了达到更好的策略学习目的，又根据网络结构和参数训练方式的不同提出了三个改进模型：双网络独立训练模型、一步联合训练混合模型和两步联合训练混合模型。

3）模型的评估与实验仿真。利用公开数据集进行模型的训练和仿真环境的构建，然后对所提出的算法从不同角度进行评估。

\section{下一步工作方向}
 为了更好的将强化学习算法应用在直复营销场景中，本文进行了很多研究，但是由于直复营销问题比较复杂、强化学习算法过于灵活难懂加之本文学术水平有限，本文的研究内容还存在很多不足之处，这些不足之处需要在未来的工作中去改进。具体如下：

 1）在本文第二个工作中，因为DQN模型的更新方式和本文第一个工作中批Q-leaning的更新方式不同，所以第一个工作中的Interval-Q算法很难直接应用到DQN模型的更新过程中，因而第二个工作没有考虑可变时间间隔的问题，所以，作者接下来的首要工作就是对第四章中所提算法进行改进，使之可以解决可变时间间隔问题。

 2）在强化学习中，仿真环境的建立至关重要，但是本文所建立的方针环境过于理想，仅仅使用于模型的评估比较，而并不能直接将仿真环境的评估结果作为实际结果使用，所以下一步的工作是进行仿真环境的研究，使之可以更好的评估模型，训练模型。

 3）与其他营销场景的结合。本文实验仿真部分，所选用的数据集来自直邮营销中的数据，而这种营销方式虽然经典，但是在实际的应用过程中周期太长。随着中国互联网广告的发展，出现了很多形式的互联网广告，而这些广告形式都是从直复营销发展而来的，所以接下来应该努力寻找更多的数据，将所研究的模型应用在定向营销、广告推荐、渠道广告投放等中国新型广告上去。