\chapter{总结与展望}

\section{本文工作总结}
针对传统的监督学习和非监督学习方法在处理直复营销问题时，只能最大化单个事件的即时收益，而无法保证在序贯决策问题中能达到长期收益最大化这一问题，本文选择使用基于值函数的强化学习方法进行研究。然后，为了更好的适应实际需求，本文针对直复营销场景中的营销决策点时间间隔不固定、数据负载大学习效率不高以及客户状态的部分可观测性等三个方面对基于值函数的强化学习算法进行改进，并使用仿真环境验证所提方法的有效性。主要工作可以总结为以下三个方面。

（1）针对直复营销场景中决策时间间隔不固定以及数据规模大导致学习效率不高这两个问题，本文基于经典的强化学习算法Q-learning进行研究，并提出了改进的Q-learning算法。具体地，使用均值标准化的方法减少决策点间的时间间隔不固定给奖赏信号带来的噪声影响，进而针对Q值函数在迭代过程中因为时间间隔更新不同步而带来的偏差，构建一个标准化因子，并仿照值函数的更新方法进行标准化因子的更新，由此提出Interval-Q算法。接着，针对Interval-Q算法在处理大规模数据时，训练速度慢，学习效率不高的问题，本文在Q采样法的基础上，引入TD偏差，提出基于TD偏差的Q采样法。

（2）针对传统强化学习算法无法有效处理直复营销场景中客户状态部分可观测的问题，本文基于深度强化学习DQN模型进行研究，并提出了基于双网络的DQN模型。具体地，首先结合营销场景的时序特点，提出通过使用基于RNN网络的DQN模型（DQN_RNN）以学习隐状态的方式来解决上述问题。然后，指出DQN_RNN模型在网络优化过程中不能很好地同时进行隐状态的学习和值函数的逼近，由此提出基于双网络的DQN模型：通过RNN网络从监督数据中学习隐状态的表示方法，再将RNN网络输出的隐状态作为DQN网络的输入状态进行强化学习，通过这种方式可以充分发挥两个网络各自的优势，在提高值函数逼近效果的同时也能更好地学习隐状态。最后，为了取得更好的策略学习效果，又从网络结构和训练方法两个角度进行分析，并提出了改进的模型。

（3）模型的评估与实验仿真。利用公开数据集进行模型的训练和仿真环境的构建，然后对所提出的算法从不同角度进行评估。具体地，针对（1）中的两个改进算法在不定期直复营销的仿真环境中进行评估，其中，对于本文提出的Interval-Q算法，从策略的长期收益和策略行为的变化情况两个角度进行评估，通过实验表明本文所提的算法因为考虑到营销时间时间间隔不固定问题，在不定期直复营销场景中，相比传统的Q-learning算法，取得了更高的收益，并且策略也相对更稳定。另外，将本文所提出的基于TD偏差的Q采样方法从长期收益和采样数两个方面与Q采样法和随机采样法进行比较，验证了所提算法在减少采样数目的同时可以取得更高的收益。最后，针对（2）中的改进模型在定期直复营销的仿真环境进行评估。从模型的长期收益上对所提算法进行评估，证明本文所提的基于双网络的DQN模型相比基本的深度强化学习模型有更好的策略效果。另外，使用不同规模的数据集对模型进行训练，实验结果表明所提模型不需要利用大批量数据进行训练同样可以产生较好的营销策略。

\section{下一步工作方向}
 为了更好地将强化学习算法应用于直复营销的场景中，本文从多个角度对基于值函数的强化学习算法进行改进。但是由于直复营销问题比较复杂、强化学习算法过于晦涩难懂加之本人学术水平有限，本文的研究内容还存在很多不足之处，这些不足之处需要在未来的工作中去改进。具体如下：

 （1）因为DQN模型和Interval-Q算法值函数的更新方式不同，所以Interval-Q算法中针对时间间隔不固定的解决方案很难直接应用到DQN模型的更新过程中，因而本文在设计基于双网络的DQN模型时并没有考虑时间间隔不固定的问题，所以，接下来的首要工作就是对第四章中所提算法进行改进，使之可以解决时间间隔不固定的营销问题，让算法更有普适性。

 （2）在强化学习中，仿真环境的建立至关重要，但是本文所建立的仿真环境过于理想，仅仅适用于模型的评估对比，并不能直接将仿真环境的评估结果作为真实应用场景的评估结果，所以下一步的工作是进行仿真环境的研究，使之可以更好的评估模型，或者研究新的评估方法来更准确的强化学习模型。

 （3）与其它营销场景的结合。在本文实验仿真部分，只选择一个直邮营销的数据集进行模型的训练，其主要原因是本文所需的数据集需要满足很多严格的要求，比如：需要具有客户持续的交互数据、需要带有奖赏标签的反馈数据以及需要具有一定的时间跨度。而且，因为涉及公司敏感信息，在目前公开的数据集中，满足这些要求数据集比较少。所以，接下来应该努力寻找相匹配的数据，进一步验证本文所提的算法并将其应用到实际的场景中。

 （4）因为基于策略搜索的强化学习方法在实际应用中具有很好的收敛性而且非常善于处理动作空间很大或者连续的实际问题，所以作者在之后的工作中将开始尝试使用基于策略搜索的强化学习方法来解决实际的应用问题。




